---
title: "Example 3"
output: html_document
date: "2024-07-07"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#We import the samle libraries from example 1 here, with identical comments. 
#Importing the sampleSelection package - This is the focal package of my 
#dissertation. 

library(sampleSelection)

#Importing the libraries ssmrob and ssmodels, which are for robust inference and 
#modelling for sample selection models.

library(ssmrob)
library(ssmodels)

#Importing packages here to apply the Hessian method. 
library(numDeriv)
```

## Data Loading and Cleaning
```{r pressure, echo=FALSE}
#Loading in the Mroz87 dataset.
dat = Mroz87
#Checking what variables are in Mroz87
str(dat)
ls(dat)
#?Mroz87
#Getting a summary of Mroz87.
summary(dat)
#As suggested by Toomet and Henningsen (2008), creating the variable kids, an
#indicator for whether the family has any kids, so if the sum of kids5 and 
#kids618 exceeds 0. 
dat$kids = (dat$kids5 + dat$kids618 > 0)
#Defining exper2 and age2 to be the square of exper and age respectively.
dat$exper2 = dat$exper^2
dat$age2 = dat$age^2
```
## Exploratory Data Analysis
```{r}
#Tobit-2 model implementation taken from Rubio (2024).
#Attaching dat
attach(dat)
#... for the selection process.
dm.sel = cbind(exper, exper2, educ, city)
#... for the observable process.
dm.obs = cbind(age, age2, faminc, kids, educ)

#Selecting the response of the selection and observable process as lfp and wage
#respectively.
resp.sel = as.logical(lfp)
resp.obs = wage

#Defining npar.sel and npar.obs as the number of non-intercept parameters in 
#the selection and observable processes respectively.
npar.sel = ncol(dm.sel); npar.obs = ncol(dm.obs)
#Defining npar as the total number of parameters in the model. 
npar = npar.sel + 1 + npar.obs + 1 + 2

#Exploratory data analysis
#Getting a summary for the selection and observable processes' non-intercept 
#covariates. 
summary(dm.sel)
summary(dm.obs)

#Plotting boxplots for 
boxplot(cbind(exper, educ), col = c("red", "blue"), 
        main = "Selection Process", xlab = "Covariate")
legend(x = "topright", legend = c("Work Experience (years)", 
        "Education (years)"), col = c("red","blue"), pch = rep(16, 2))
boxplot(age, col = c("green"), main = "Age", xlab = "Age", ylab = "Age (years)")
boxplot(faminc, col = c("brown"), main = "Family Income", xlab = "Income", 
        ylab = "USD(1975)")

#Getting the proportion of city, kids and missingness as 1 - lfp, labour force 
#participation. 

mean(city)
mean(kids)
1-mean(lfp)
```

## Fitting the Tobit-2 Model

```{r}
#Below is my implementation of the Tobit2 MLE, MLETobit2, from example 1. 
#This takes inputs init, the inital value from where the parameter estimate is 
#evaluated from, out, the responses of the observable model, sel, the responses 
#of the selection model, deso, the design matrix for the observable model, less 
#the first column, dess, the design matrix for the selection process less the 
#first column, method, the optimisation method, and maxit, the maximum number of
#iterations, with default number 100.

MLETobit2 = function(init, out, sel, deso, dess, beta_par = 0,
                     method = "nlminb", maxit = 100){
  #Converting the inputs out and sel into vectors.
  out = as.vector(out); sel = as.vector(sel)
  #Defining x1 and w1 as the design matrices of the observable and selection 
  #processes respectively by adding back the missing column of 1's.
  x1 = as.matrix(cbind(1, deso)); w1 = as.matrix(cbind(1, dess))
  #Defining p and q as the number of columns of x1 and w1 respectively. 
  p = ncol(x1); q = ncol(w1)
  #Defining a function which evaluates the log-likelihood of the Tobit-2 model,
  #loglik. This takes in as inputs a parameter vector, par. 
  loglik = function(par){
    #Reparametrising sigma and rho in our model, using an exponential and a 
    #tanh link. 
    sigma = exp(par[1]); rho = tanh(par[2]) 
    #Defining our linear regression coefficients for the observable model, beta, 
    #and that for the selection model, alpha. (Modified from Rubio(2023)).
    #TODO: Reparametrise beta = beta.tilde
    beta = par[c(3:(p+2))]; alpha = par[(p+3):(p+q+2)]
    #Reparametrisation if beta_par = 1
    if(beta_par){
      beta = beta/sigma
    }
    #Defining the linear predictors of the observable process (x_beta) and the 
    #the selection process (w_alpha). 
    x_beta = x1%*%beta; w_alpha = w1%*%alpha
    #Defining inds0 and inda1 as indicators corresponding to the selection 
    #criterion, which can be seen in chapter 2. 
    inds0 = (sel == 0); inds1 = (sel == 1)
    #Evaluating the log-likelihood of the Tobit-2 model as given in Chapter 2. 
    ll = sum(pnorm(-w_alpha[inds0], log = T)) + 
         sum(pnorm( (w_alpha[inds1] + rho*(out[inds1] - x_beta[inds1])/sigma)/
                      sqrt(1 - rho^2), log = T)) + 
    #MODIFICATION: compressed the explicit evaluation of the normal component of
    #the observable processes into the dnorm function, as the distribution is a
    #N(x_beta, sigma^2) distributed random variable from proof (INSERT). 
         sum(dnorm(out[inds1], mean = x_beta[inds1], sd = sigma, log = T))
    #Returning the log-loss, which we will minimise using the optim commands. 
    #This is because optim and nlminb are principally minimisation algorithms, 
    #so returning the log-loss is more efficient here. 
    return(-ll)
  }
  #Conditional to evaluate the optimal value using nlminb if specified
  #(by default).
  if(method == "nlminb"){ 
    OPT = nlminb(init, loglik, control = list(iter.max = maxit))
  }
  #MODIFICATION - Conditional to evaluate the optimal value using the optim() 
  #function
  else{
    #Using optim to allow us to select our desired optimisation algorithm.
    OPT = optim(init, loglik, control = list(maxit = maxit), method = method)
  }
  #Storing the MLES in a vector, remembering to undo the links defined above. 
  MLE = c(exp(OPT$par[1]), tanh(OPT$par[2]), OPT$par[-c(1,2)])
  #Creating a vector of estimate names...
  names(MLE) = c("sigma", "rho", "intercept", paste(paste("beta_hat[,", 1:(p-1), 
    sep = "" ),"]", sep = ""), "intercept", paste(paste("alpha_hat[,", 1:(q-1), 
    sep = "" ),"]", sep = "")) 
  #Storing all our outputs in the list outf.
  outf = list(loglik = loglik, OPT = OPT, MLE = MLE)
  #Returning outf as the function output. 
  return(outf)
  
}

#Find the MLEs, using MLETobit2, at initial point 0. 
OPT0 = MLETobit2(init = rep(0, npar), out = resp.obs, sel = resp.sel, 
                deso = dm.obs, dess = dm.sel, maxit = 1E4)

#Returning the Tobit-2 MLEs and the log likelihood as the negative of the 
#log-loss.
OPT0$MLE
-OPT0$OPT$objective

#Repeating the same procedure except for initial point 1 for all parameters.
OPT2 = MLETobit2(init = rep(1, npar), out = resp.obs, sel = resp.sel, 
                deso = dm.obs, dess = dm.sel, maxit = 1E4)
OPT2$MLE
-OPT2$OPT$objective

#Creating the observable and selection process as equations to compare against
#selection() in Toomet and Henningsen (2008) and HeckmanCL() in de Souza Bastos 
#and Barreto de Souza (2022), respectively. We fit the models then print a 
#summary and their log-likelihood,
obsEq = wage ~ exper + I(exper^2) + educ + city
selectEq = lfp ~ age + I(age^2) + faminc + kids + educ

ssMLE = selection(selectEq, obsEq, data = dat, method = "ml")
-ssMLE

#Also comparing against the results of the 2-step estimator.
ssMLE.2step = selection(selectEq, obsEq, data = dat, method = "2step")
ssMLE.2step 

#NOTE: Starting point is set at the MLE of the selection process.
HMLE = HeckmanCL(selectEq, obsEq, data = dat, start = coef(ssMLE))
HMLE$loglik

ssMLE.zero = selection(selectEq, obsEq, start = c(rep(0, 11), 0.5, 0.9), 
                       data = dat)
ssMLE.zero$maximum

```

## Profile Likelihood
```{r} 
#Defining OPT to be the output from the MLETobit2() function with zero initial 
#point, p to be the number of parameters in the MLE, ML to be the log-loss of 
#model, and MLE to be the MLE in OPT.  
OPT = OPT0
p = length(OPT$MLE)
ML = OPT$OPT$objective
MLE = OPT$MLE

#Insert profile likelihood from example 1 with comments.
#Create the function prof.lik to evaluate the profile likelihood MLE. 
#This has parameters parint, for the parameter of interest (or its
#reparametrisations), ind, the corresponding index, init, the initial value used
#for optimisation, either from zeros or from the regular MLE, max.iter, the 
#number of iterations, and method, the optimisation algorithm used.
#MODIFICATIONS: Added method and max.iter as new parameters. 
prof.lik = function(parint, ind, init = "zero", max.iter = 1E4, method = "nlminb"){
  #Creating the tempf function as a dummy function to perform optimisation over.
  tempf = function(par){
    tempv = rep(0, p)
    tempv = replace(x = tempv, c(1:p)[-ind], par)
    tempv[ind] = parint
    return(OPT$loglik(tempv))
  }
  #MODIFICATION - condensed the two conditional statements into a single 
  #conditional using truthy values & their correspondence to 0/1.
  #MODIFICATION - Add the max.iter variable here
  if (method == "nlminb"){
    #Evaluate the output out as the negative of the nlimb() optimisation output, 
    #since this will mean we get a maximum, as nlminb() finds minima. 
  out = -nlminb(OPT$OPT$par[-ind]*rep(init == "MLE", length(OPT$OPT$par[-ind])), 
                tempf, control = list(iter.max = max.iter))$objective + ML
  }
  #MODIFICATION - Add a conditional statement for any non-nlminb method 
  #supported in optim. 
  else{
    #Evauate the output out as the negative of the optim() minimisation output
    #to get a maximum estimate.
    out = -optim(par = OPT$OPT$par[-ind]*rep(init == "MLE", 
      length(OPT$OPT$par[-ind])), fn = tempf, method = method, control = 
      list(maxit = max.iter, reltol = 1E-5))$value + ML
  }
  #We return the exponential of the output since we used the log-likelihood.
  #This returns the profile likelihood. 
  return(exp(out))
}

#This is specifically for the degenerate example. 
#Creating the Vectorize() object prof1degen to dynamically evaluate the 
#Profile likelihood. 
prof1degen = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, 
                                         init = "MLE", method = "nlminb"))
#Using the curve() function to plot out the profile likelihood. 
curve(prof1degen, n = 100, from = 0.01, to = 1.1, ylab = "Profile Likelihood", 
      xlab = expression(sigma), main = "zero-nlminb")
#Adding a line to signify the MLE. 
abline(v = OPT$MLE[1], col = "red", lwd = 2)
#Adding a legend for clarity. 
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

################################################################################
#Performing the same definitions as in the previous snippet, but using OPT2 this
#time.
OPT = OPT2
p = length(OPT$MLE)
ML = OPT$OPT$objective
MLE = OPT$MLE

#Performing the same procedure as in prof1degen for ALL parameters in the 
#Tobit-2 model using initial point 1 in all dimensions (OPT2). 
prof1 = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, 
                                         init = "MLE", method = "nlminb"))
curve(prof1, n = 100, from = 3, to = 5, ylab = "Profile Likelihood", 
      xlab = expression(sigma), 
      main = "one-nlminb")
abline(v = OPT$MLE[1], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

#par(mfrow = c(2,2))

prof2 = Vectorize(function(par) prof.lik(parint = atanh(par), ind = 2, 
                                          init = "MLE", method = "nlminb"))
curve(prof2, n = 100, from = 0.96, to = 0.9999, ylab = "Profile Likelihood", 
      xlab = expression(rho) 
      )
abline(v = OPT$MLE[2], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof3 = Vectorize(function(par) prof.lik(parint = par, ind = 3, init = "MLE",
                                         method = "SANN"))
curve(prof3, n = 100, from = -10.5, to = -9.5, ylab = "Profile Likelihood", 
      xlab = expression(beta["int"])
      )
abline(v = OPT$MLE[3], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof4 = Vectorize(function(par) prof.lik(parint = par, ind = 4, init = "MLE",
                                         method = "nlminb"))
curve(prof4, n = 100, from = -1, to = 1.6, ylab = "Profile Likelihood", 
      xlab = expression(beta["age"])
      )
abline(v = OPT$MLE[4], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof5 = Vectorize(function(par) prof.lik(parint = par, ind = 5, init = "MLE",
                                         method = "BFGS"))
curve(prof5, n = 100, from = -0.0025, to = -0.001, ylab = "Profile Likelihood", 
      xlab = expression(beta["age2"])
      )
abline(v = OPT$MLE[5], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof6 = Vectorize(function(par) prof.lik(parint = par, ind = 6, init = "MLE",
                                         method = "nlminb"))
curve(prof6, n = 200, from = 0, to = 0.0001, ylab = "Profile Likelihood", 
      xlab = expression(beta["faminc"])
      )
abline(v = OPT$MLE[6], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof7 = Vectorize(function(par) prof.lik(parint = par, ind = 7, init = "MLE",
                                         method = "BFGS"))
curve(prof7, n = 100, from = -1, to = 0.1, ylab = "Profile Likelihood", 
      xlab = expression(beta["kids"])
      )
abline(v = OPT$MLE[7], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof8 = Vectorize(function(par) prof.lik(parint = par, ind = 8, init = "MLE",
                                         method = "SANN"))
curve(prof8, n = 100, from = 0.53, to = 0.63, ylab = "Profile Likelihood", 
      xlab = expression(beta["educ"])
      )
abline(v = OPT$MLE[8], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof9 = Vectorize(function(par) prof.lik(parint = par, ind = 9, init = "MLE",
                                         method = "BFGS"))
curve(prof9, n = 100, from = -1.9, to = -1.6, ylab = "Profile Likelihood", 
      xlab = expression(alpha["int"])
      )
abline(v = OPT$MLE[9], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof10 = Vectorize(function(par) prof.lik(parint = par, ind = 10, init = "MLE",
                                         method = "nlminb"))
curve(prof10, n = 100, from = 0, to = 0.025, ylab = "Profile Likelihood", 
      xlab = expression(alpha["exper"])
      )
abline(v = OPT$MLE[10], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof11 = Vectorize(function(par) prof.lik(parint = par, ind = 11, init = "MLE",
                                         method = "nlminb"))
curve(prof11, n = 100, from = -0.0005, to = 0.0005, ylab = "Profile Likelihood", 
      xlab = expression(alpha["exper2"])
      )
abline(v = OPT$MLE[11], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof12 = Vectorize(function(par) prof.lik(parint = par, ind = 12, init = "MLE",
                                         method = "BFGS"))
curve(prof12, n = 100, from = 0.14, to = 0.16, ylab = "Profile Likelihood", 
      xlab = expression(alpha["educ"])
      )
abline(v = OPT$MLE[12], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof13 = Vectorize(function(par) prof.lik(parint = par, ind = 13, init = "MLE",
                                         method = "Nelder-Mead"))
curve(prof13, n = 100, from = -0.25, to = 0.25, ylab = "Profile Likelihood", 
      xlab = expression(alpha["city"])
      )
abline(v = OPT$MLE[13], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

#par(mfrow = c(1,1))

```

## Applying the Hessian Method
```{r}
#Performing the procedure as described in Cole (2020), pp. 72-73.
#Evaluating the Hessian at the numerically-obtained MLE. 
hess.model = -hessian(func = OPT$loglik, x = OPT$OPT$par)
#Getting the Hessian's eigenvalues.
eigen.vals = eigen(hess.model)$values
#Getting the reference values as defined in Cole (2020) and in Chapter 5 of the 
#Dissertation.
ref.vals = abs(as.vector(eigen.vals))/max(abs(as.vector(eigen.vals)))
#Getting sev as the sorted vector of reference values. 
sev = sort(ref.vals)
sev
#Checking if the larges reference value is lower than the 0.001 threshold. 
sev[1] < 0.001

#Setting the seed here to be 1111 for replicability.
set.seed(1111)
#Below is the implementation for the Bootstrap on the Hessian method adapted 
#from Rubio, Espindola and Montoya (2023).
#Defining the number of bootstrap samples, B, to be 1000. 
B = 1E3
n = nrow(dat)
#Defining an empty vector, MLE.B, which will contain all of the bootstrapped
#MLEs for all B = 1000 samples.
MLE.B = matrix(0, ncol = length(MLE), nrow = B)
#Defining a completely empty vector indHess, which will show the result of 
#performing the above. 
indHess = vector()
#For loop iterating from index 1 to B, to perform the bootstrap.  
for (i in 1:B){
  #Deciding which observation (i.e. index) from dat we will use to bootstrap,
  #and using sample() to get the index to be bootstrapped over, ind.
  ind = sample(1:n, replace = T)
  #Using GHMLE() again to fit the model, just like for OPTPGWGH, except 
  #only considering the inputs corresponding to observatoin ind.
  OPTB = MLETobit2(init = rep(1, npar), out = resp.obs[ind], 
                   sel = resp.sel[ind], deso = dm.obs[ind,], 
                   dess = dm.sel[ind,], maxit = 1E4)
  MLE.B[i, ] = c(exp(OPTB$OPT$par[1]), tanh(OPTB$OPT$par[2]), 
                 OPTB$OPT$par[-c(1:2)])
  #The following code is just the implementation from above of the Hessian 
  #method - to avoid bloat, refer to the comments from the preceeding section.
  
  #This was originally a bug, where it evaluated the hessian using the wrong 
  #optimised output - I found it, and this is mentioned in the corresponding 
  #RPub (https://rpubs.com/FJRubio/NRPNILung) as a disclaimer.
  hessb = -hessian(OPTB$loglik, x = OPTB$OPT$par)
  eigen.val.b = eigen(hessb)$values
  ref.val.b = abs(as.vector(eigen.val.b))/abs(max(as.vector(eigen.val.b)))
  sev.b = sort(ref.val.b)
  indHess[i] = as.numeric(sev.b[1]<0.001)
}
#Getting the sample proportion of parameter-redundant numerical models as the 
#mean of the indHess vector. 
mean(indHess)
```
## Creating Confidence Intervals

```{r}

#Implementing the profile likelihood based interval for sigma (inspired by
#Rubio (2024)).
#Defining our significance level, SL = 0.05
SL = 0.05
#Defining bound following Lemma 4.1 in my dissertation. 
bound = exp(-0.5*qchisq(1-SL, df = 1))
#Defining intprof1 as the pro
#Evaluating the lower bound and upper bound using the uniroot() function as LP
#and UP respectuvely.
intprof1 = Vectorize(function(sigma) prof1(sigma) - bound)
LP = uniroot(f = intprof1, interval = c(3.7, 4.1))$root
UP = uniroot(f = intprof1, interval = c(4.2, 4.5))$root
#Reversing the log link for sigma to get the profile likelihood confidence 
#interval, confint1.proflik
confint1.proflik = exp(c(LP, UP))

#Performing the LP-UP procedure again for all remaining parameters, as in 
#Example 2. 

#This will be done iteratively, so we define the vectors LPVec and UPVec
#just as in Example 2, with length p-1 each. 

LPVec = rep(NA, p-1)
UPVec = rep(NA, p-1)

#NOTE FOR RHO - We also invert the tanh link here. 
intprof2 = Vectorize(function(rho) prof2(rho) - bound)
LPVec[1] = uniroot(f = intprof2, interval = c(0.98, 0.99))$root
UPVec[1] = uniroot(f = intprof2, interval = c(0.99, 0.999999))$root

confint2.proflik = tanh(c(LPVec[1], UPVec[1]))

intprof3 = Vectorize(function(beta) prof3(beta) - bound)
LPVec[2] = uniroot(f = intprof3, interval = c(-10.5, -10))$root
UPVec[2] = uniroot(f = intprof3, interval = c(-9.8, -9.5))$root

intprof4 = Vectorize(function(beta) prof4(beta) - bound)
LPVec[3] = uniroot(f = intprof4, interval = c(-0.3, 0.0))$root
UPVec[3] = uniroot(f = intprof4, interval = c(0.2, 0.5))$root

intprof5 = Vectorize(function(beta) prof5(beta) - bound)
LPVec[4] = uniroot(f = intprof5, interval = c(-0.01, -0.005))$root
UPVec[4] = uniroot(f = intprof5, interval = c(0, 0.03))$root

intprof6 = Vectorize(function(beta) prof6(beta) - bound)
LPVec[5] = uniroot(f = intprof6, interval = c(0, 0.00003))$root
UPVec[5] = uniroot(f = intprof6, interval = c(0.00005, 0.0001))$root

intprof7 = Vectorize(function(beta) prof7(beta) - bound)
LPVec[6] = uniroot(f = intprof7, interval = c(-0.7, -0.5))$root
UPVec[6] = uniroot(f = intprof7, interval = c(-0.3, 0))$root

intprof8 = Vectorize(function(beta) prof8(beta) - bound)
LPVec[7] = uniroot(f = intprof8, interval = c(0.3, 0.5))$root
UPVec[7] = uniroot(f = intprof8, interval = c(0.7, 0.8))$root

intprof9 = Vectorize(function(alpha) prof9(alpha) - bound)
LPVec[8] = uniroot(f = intprof9, interval = c(-1.9, -1.8))$root
UPVec[8] = uniroot(f = intprof9, interval = c(-1.8, -1.7))$root

intprof10 = Vectorize(function(alpha) prof10(alpha) - bound)
LPVec[9] = uniroot(f = intprof10, interval = c(-0.01, 0.01))$root
UPVec[9] = uniroot(f = intprof10, interval = c(0.01, 0.02))$root

intprof11 = Vectorize(function(alpha) prof11(alpha) - bound)
LPVec[10] = uniroot(f = intprof11, interval = c(-0.0005, -0.0004))$root
UPVec[10] = uniroot(f = intprof11, interval = c(-0.0003, 0))$root

intprof12 = Vectorize(function(alpha) prof12(alpha) - bound)
LPVec[11] = uniroot(f = intprof12, interval = c(0.14, 0.145))$root
UPVec[11] = uniroot(f = intprof12, interval = c(0.15, 0.16))$root

intprof13 = Vectorize(function(alpha) prof13(alpha) - bound)
LPVec[12] = uniroot(f = intprof13, interval = c(-0.2, 0))$root
UPVec[12] = uniroot(f = intprof13, interval = c(0.05, 0.1))$root

for(i in 2:length(LPVec)){
  assign(paste("confint", i+1, ".proflik", sep = ""), c(LPVec[i], UPVec[i]))
}
#MODIFICATION: Bootstrapped confidence intervals. 

#Just in case, setting seed for replicability. 
set.seed(1111)

#Running a for loop which creates numerical central 95% percentiles for each 
#MLE using the quantile() function, and assigning it with the assign function()
#to dynamically create variables confint"par number". 
for (i in 1:length(MLE)){
  assign(paste("confint.boot.", i, sep = ""), 
         quantile(MLE.B[, i], probs = c(0.025, 0.975)))
}

#The following is the implemetation for Bias Corrected-Accelerated confidence 
#intervals. 
#This follows the methods introduced throughout DiCiccio and Efron (1996).

#Defining n to be the number of observations(rows in the dat dataframe).
n = nrow(dat)

#Evaluating the Jackknife influence functions so we can evaluate a.hat.
#The Jackknife is used to estimate the acceleration parameter, as defined in 
#DiCiccio and Efron (1996).

jackknives = matrix(nrow = n, ncol = length(MLE))

#We do this iteratively. 
#WARNING: This loop takes a long time to run - around 15-20 minutes. 
#This is due to how the Jackknife is calculated, and there is no possible
#optimisation unfortunately.

for (i in 1:n){
  #Getting the MLEs for the Tobit-2 model, omitting the ith observation, and 
  #storing this in the variable OPT_i. For more details see the above section
  #where we evaluated all the MLEs in the regular case. 
  OPT_i = MLETobit2(init = rep(1, npar), out = resp.obs[-i], 
                    sel = resp.sel[-i], deso = dm.obs[-i, ], 
                    dess = dm.sel[-i, ], maxit = 1E4)
  #Getting the MLEs without i, as detailed above in the regular case. 
  MLE_i = c(exp(OPTB$OPT$par[1]), tanh(OPTB$OPT$par[2]), 
               OPTB$OPT$par[-c(1:2)])
  #Evaluating the Jackknife.
  jackknives[i, ] = (n-1)*(MLE - MLE_i)
}

#Create the function BC_a.ci, which takes in parameters 
#B, the number of bootstrap samples, MLE, the MLE VECTOR of the data, 
#ind, the index determining which MLE to create the confidence interval for,  
#alpha, the significance level of the confidence interval, booted.MLes, the 
#matrix of bootstrapped MLEs, and jackknives, the matrix of jackknife influence
#functions, as defined above. 

BC_a.ci = function(B, MLE, ind, alpha, booted.MLEs, jackknives){
  #Evaluate z_0.hat as in DiCiccio and Efron (1996). 
  z_0.hat = qnorm(1/B*sum(booted.MLEs[,ind] < MLE[ind]))
  #Defining a.hat as in DiCiccio and Efron (1996), and evaluating the lower 
  #bound (LB) and upper bound (UB) of the BC_a confidence interval.
  a.hat = 1/6*sum(jackknives[,ind]^3)/(sum(jackknives[,ind]^2)^1.5)
  LB = quantile(x = booted.MLEs[,ind], probs = pnorm(z_0.hat + (z_0.hat + 
          qnorm(alpha/2 )/(1 - a.hat*(z_0.hat + qnorm(alpha/2) ) ) ) ) )
  UB = quantile(x = booted.MLEs[,ind], probs = pnorm(z_0.hat + (z_0.hat + 
          qnorm(1 - alpha/2 )/(1 - a.hat*(z_0.hat + qnorm(1-alpha/2) ) ) ) ) )
  #Defining the variable bootstrap.CI to be our BC-A CI.
  bootstrap.CI = c(LB, UB)
  #Returning bootstrap.CI. 
  return(bootstrap.CI)
}

#Iterates over all the parameters. 
for (i in 1:length(MLE)){
  #Runs BC_a.ci(), iterating over the iterate, i, for all parameters. 
  #Then, parses this to the assign() function along with paste(), in order 
  #to dynamically store my outputs as confint.bc_a"parameter no."
  assign(paste("confint.bc_a", i, sep = ""), BC_a.ci(B, MLE, i, 0.05, MLE.B, 
                                                    jackknives))
}
```