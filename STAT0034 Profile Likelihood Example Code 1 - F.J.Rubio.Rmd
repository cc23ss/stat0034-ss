---
title: "Untitled"
output: html_document
date: "2024-06-02"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Formulation of the Tobit-2 log-likelihood (Ch. 2)

```{r}
#The following implementation was taken from Rubio (2024)
#ADDITION: setting the seed for replicability. 
set.seed(1111)
#Importing the sampleSelection package - This is the focal package of my 
#dissertation. 

library(sampleSelection)

#Importing the libraries ssmrob and ssmodels, which are for robust inference and 
#modelling for sample selection models.

library(ssmrob)
library(ssmodels)

#Defining the MLE for Tobit-2 model as the function MLETobit2, as discussed in 
#Chapter 2 of my dissertation.

#This takes inputs init, the inital value from where the parameter estimate is 
#evaluated from, out, the responses of the observed model, sel, the responses 
#of the selection model, deso, the design matrix for the observed model, less the 
#first column, dess, the design matrix for the selection process less the first 
#column, method, the optimisation method, and maxit, the maximum number of 
#iterations, with default number 100.

MLETobit2 = function(init, out, sel, deso, dess, beta_par = 0,
                     method = "nlminb", maxit = 100){
  #Converting the inputs out and sel into vectors.
  out = as.vector(out); sel = as.vector(sel)
  #Defining x1 and w1 as the design matrices of the observed and selection 
  #processes respectively by adding back the missing column of 1's.
  x1 = as.matrix(cbind(1, deso)); w1 = as.matrix(cbind(1, dess))
  #Defining p and q as the number of columns of x1 and w1 respectively. 
  p = ncol(x1); q = ncol(w1)
  #Defining a function which evaluates the log-likelihood of the Tobit-2 model,
  #loglik. This takes in as inputs a parameter vector, par. 
  loglik = function(par){
    #Reparametrising sigma and rho in our model, using an exponential and a 
    #tanh link. 
    sigma = exp(par[1]); rho = tanh(par[2]) 
    #Defining our linear regression coefficients for the observed model, beta, 
    #and that for the selection model, alpha. 
    #TODO: Reparametrise beta = beta.tilde
    beta = par[c(3:(p+2))]; alpha = par[(p+3):(p+q+2)]
    #Reparametrisation if beta_par = 1
    if(beta_par){
      beta = beta/sigma
    }
    x_beta = x1%*%beta; w_alpha = w1%*%alpha
    #Defining inds0 and inda1 as indicators corresponding to the selection 
    #criterion, which can be seen in chapter 2. 
    inds0 = (sel == 0); inds1 = (sel == 1)
    #Evaluating the log-likelihood of the Tobit-2 model as given in Chapter 2. 
    ll = sum(pnorm(-w_alpha[inds0], log = T)) + 
         sum(pnorm( (w_alpha[inds1] + rho*(out[inds1] - x_beta[inds1])/sigma)/
                      sqrt(1 - rho^2), log = T)) + 
    #MODIFICATION: compressed the explicit evaluation of the normal component of
    #the observed processes into the dnorm function, as the distribution is a
    #N(x_beta, sigma^2) distributed random variable from proof (INSERT). 
         sum(dnorm(out[inds1], mean = x_beta[inds1], sd = sigma, log = T))
    #Returning the log-loss, which is the objective we seek to minimise. 
    return(-ll)
  }
  #Conditional to evaluate the optimal value using nlminb if specified
  #(by default).
  if(method == "nlminb"){ 
    OPT = nlminb(init, loglik, control = list(iter.max = maxit))
  } 
  #Conditional to evaluate the optimal value using the optim() function
  else{
    #Using optim to 
    OPT = optim(init, loglik, control = list(maxit = maxit), method = method)
  }
  #Storing the MLES in a vector, remembering to undo the links defined above. 
  MLE = c(exp(OPT$par[1]), tanh(OPT$par[2]), OPT$par[-c(1,2)])
  #Creating a vector of estimate names...
  names(MLE) = c("sigma", "rho", "intercept", paste(paste("beta_hat[,", 1:(p-1), 
    sep = "" ),"]", sep = ""), "intercept", paste(paste("alpha_hat[,", 1:(q-1), 
    sep = "" ),"]", sep = "")) 
  #Storing all our outputs in the list outf.
  outf = list(loglik = loglik, OPT = OPT, MLE = MLE)
  #Returning outf as the function output. 
  return(outf)
  
}
```

## Example 1 - Profile Likelihood, applied onto MEPS2001

We use this example from (Rubio 2024a) as an example using the profile 
likelihood on a sample selection model, with modifications stated where 
necessary both above and below. 

```{r MEPS2001, echo=FALSE}
#Attach the MEPS2001 dataset from the ssmrob package. 
attach(MEPS2001)

#Beginning of exploratory data analysis. 
#?MEPS2001 - Read documentation for this dataset.
#Look at the top of
head(MEPS2001)

#TODO: Exploratory data analysis. 

#Defining design matrices (less the intercept column). 
#TODO: Change influential variables? 
#... for the observed model.
dm.obs = cbind(age, female, educ, blhisp, totchr, ins)
boxplot(dm.obs[, c(1, 3, 5)])
pairs(dm.obs[, c(1, 3, 5)])
summary(dm.obs)

#... for the selection model.
#TODO: Change the issue of double-counting covariates here. 
dm.sel1 = cbind(dm.obs, income)
boxplot(dm.sel1[, c(1, 3, 5, 7)])
pairs(dm.sel1[, c(1, 3, 5, 7)])
summary(dm.sel1)
#TODO: EDA
#dm.sel2 = cbind(income, ..., )
#... for the selection model, with no overlapping covariates
dm.sel2 = MEPS2001[, 1]


#Selection response, a binary (0/1) variable which acts as the indicator 
#function corresponding to the latent variable for positive ambulatory 
#expenditures. 

sel = as.logical(dambexp)

#Observed responses, corresponding to log ambulatory expenditures, lnambx.
y = lnambx

#Finding the MLE of the Tobit-2 likelihood, with unscaled parameter. 
OPT = MLETobit2(init = rep(0, 17), out = y, sel = sel, deso = dm.obs, 
                dess = dm.sel1, method = "nlminb", maxit = 1E4, beta_par = 0)

MLE = OPT$MLE
MLE

#Doing the same thing, but with scaled beta parameter.
OPT2 = MLETobit2(init = rep(0, 17), out = y, sel = sel, deso = dm.obs, 
                dess = dm.sel1, method = "nlminb", maxit = 1E4, beta_par = 1)

MLE2 = OPT2$MLE
MLE2
MLE
#Comparing this to other optimisation algorithms. 
selectEq = dambexp ~ age + female + educ + blhisp + totchr + ins + income
obsEq1 = update(selectEq, . - dambexp + y ~ . - income)

HMLE = HeckmanCL(selectEq, obsEq1, data = MEPS2001)
summary(HMLE)

#TODO: Second round. 
summary(ssMLE)

#TODO: Alter rho from |rho| = 0 to |rho| = 1, to see the effects of correlation. 

```
## Applying the profile likelihood
```{r}
po = ncol(dm.obs); ps = ncol(dm.sel2)

p = length(MLE)
ML = OPT$OPT$objective

#Create the function prof.lik to evaluate the profile likelihood. 
#This has parameters parint, for the parameter of interest (or its
#reparametrisations), ind, the corresponding index, init, the initial value used
#for optimisation, either from zeros or from the regular MLE, max.iter, the 
#number of iterations, and method, the optimisation algorithm used.
#MODIFICATIONS: Added method and max.iter as new parameters. 
prof.lik = function(parint, ind, init = "zero", max.iter = 1E4, method = "nlminb"){
#TODO: ADD COMMENTS
  #Creating the tempf function to perform 
  tempf = function(par){
    tempv = rep(0, p)
    tempv = replace(x = tempv, c(1:p)[-ind], par)
    tempv[ind] = parint
    return(OPT$loglik(tempv))
  }
  #MODIFICATION - condensed the two conditional statements into a single 
  #conditional using truthy values & their correspondence to 0/1.
  #MODIFICATION - Add the max.iter variable here
  if (method == "nlminb"){
    #Evaluate the output out as the negative of the nlimb() optimisation output, 
    #since this will mean we get a maximum, as nlminb() finds minima. 
  out = -nlminb(OPT$OPT$par[-ind]*rep(init == "MLE", length(OPT$OPT$par[-ind])), 
                tempf, control = list(iter.max = max.iter))$objective + ML
  }
  #MODIFICATION - Add a conditional statement for any non-nlminb method 
  #supported in optim. 
  else{
    #Evauate the output out as the negative of the optim() minimisation output
    #to get a maximum estimate.
    out = -optim(par = OPT$OPT$par[-ind]*rep(init == "MLE", 
      length(OPT$OPT$par[-ind])), fn = tempf, method = method, control = 
      list(maxit = max.iter, reltol = 1E-5))$value + ML
  }
  #We return the exponential of the output since we used the log-likelihood.
  #This returns the profile likelihood. 
  return(exp(out))
}

#Interpreted from ?Vectorize call - create a vector-function hybrid object where 
#if you pass it vector values it will output a vector of corresponding function
#outputs. In this case, one can imagine here that it creates a vector object for 
#our previously defined prof.lik function, and when we pass a vector to it, it 
#should create a vector of the outputs of the prof.lik() function for all 
#elements in the vector. 

#Recall here we used an exponential link for sigma, the first parameter, so we
#revert it here. 
prof1 = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, 
                                         init = "zero"))

#Plots the profile likelihood using the curve() function. 
#NOTE: This takes a long time to run. 
curve(prof1, 1.2, 1.35, n = 50, lwd = 2, xlab = expression(sigma), 
      ylab = "Profile Likelihood")

#Perform the same procedure from above, but start with the MLE as an initial 
#value. 
prof1a = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, 
  init = "MLE", method = "BFGS"))

curve(prof1a, 1.2, 1.35, n = 50, lwd = 2, xlab = expression(sigma), 
      ylab = "Profile Likelihood")

#COMMENT: very interesting that the smoothness is so much less - some 
#investigation performed to see if it wasn't a numerical limitation below. 

#Perform the same procedure as for prof1a, but this time using 100k iterations
#instead of 10k to diagnose the roughness of the MLE profile likelihood curve. 
#prof1b = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, 
#  init = "zero", max.iter = 1E5, method = "BFGS"))

#curve(prof1b, 1.2, 1.35, n = 50, lwd = 2, xlab = expression(sigma), 
#      ylab = "Profile Likelihood")
 
#Same result - not to do with runtime. 
#However, it turns out the problem is to do with the optimisation algorithm. 
#Putting a method which isn't "nlminb" will give non-degenerate results. 

#Performing the same procedure as in prof1a, except this time for rho.
#Note that as we used a tanh link for the rho parameter, we need to invert the
#transformation when plotting the profile likelihood.
prof2 = Vectorize(function(par) prof.lik(parint = atanh(par), ind = 2, 
                                    init = "MLE", method = "nlminb"))

curve(prof2,-0.6, 0.25 , n = 50, lwd = 2, xlab = expression(rho), ylab = "Profile Likelihood")

#Performing the same procedure here as well. 
prof3 = Vectorize(function(par) prof.lik(parint = par, ind = 3, init = "MLE"))
curve(prof3,4.25,6 , n = 50, lwd = 2, xlab = expression(beta[0]), ylab = "Profile Likelihood")

```
### Constructing a Profile Likelihood Confidence Interval
```{r}
#Vectorise a shifted version of the rho parameter (parameter 2), to create a
#0.147-profile likelihood confidence interval - see Chapter 3 for more details.
intprof2 = Vectorize(function(rho) prof2(rho) - 0.147)
intprof2
#Plotting this new profile log-likelihood with the curve() function
curve(prof2,-0.6,0.25, n = 50, lwd = 2, xlab = expression(rho), ylab = "Profile Likelihood")
#... then adding the line corresponding to 0.147 to show the confidence interval
#of interest. 
abline(h = 0.147, lwd = 2, col = "red")

#Evaluating the lower and upper profile likelihood confidence interval, LP and
#UP respectively, for a 0.147-confidence interval.  

LP = uniroot(f = intprof2, interval = c(-0.5, -0.4))$root
UP = uniroot(f = intprof2, interval = c(0.1,0.2))$root
#Inverting the tanh link for rho to get a 0.147-profile likelihood confidence
#interval.
tanh(c(LP, UP))


#Detach the MEPS2001 dataset. 
detach(MEPS2001)
```