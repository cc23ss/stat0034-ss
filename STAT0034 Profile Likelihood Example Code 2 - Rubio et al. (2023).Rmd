---
title: "Example 2"
output: html_notebook
date: "07-07-2024"
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
#we import the libraries knitr, numDeriv, survival, flexsurv, devtools and 
#HazReg (HazReg was made by Rubio (UHHHHHH I NEED TO FIND HOW TO CITE THIS))
library(knitr)
library(numDeriv)
library(survival)
library(flexsurv)
library(devtools)
#The devtools package lets us use install_github("FJRubio67/HazReg") which 
#installs the HazReg package. 
#install_github("FJRubio67/HazReg")
library(HazReg)

```

```{r}
#Setting the seed as we will use a random method below (bootstrapping)
set.seed(1111)
#Attaching the lung dataset, from the survival package, and using ?lung to get
#an overview of the dataset.
#?lung
attach(lung)
#Data Cleaning. 
#removing observation 14, which was suggested in Rubio et.al. (2023) to be
#missing, and storing this in the variable dat. 
dat = lung[-14, ]

#TODO: Ask? 

#Creating a new design matrix, x, which contains the covariates age, sex 
#(reparametrised to be binary 0/1), and ECOG performance.
x = as.matrix(cbind(scale(dat$age), dat$sex - 1, scale(dat$ph.ecog)))

#Getting the number of observations considered in x.  
n = nrow(x)

#Isolating the age into xt.  
xt = as.matrix(x[,1])

#Getting the status variable from dat, and changing it into a 0/1 binary 
#variable. 
status = dat$status - 1

#Getting survival times in years from dat by dividing by 365days/year.
#For completeness, although there are no zero times, we perform a correction 
#here such that zero times are recorded as 1 day. 
#To check that there are no zeros, we use which(times == 0), which returns 
#integer(0). 
times = ifelse(dat$time == 0, 1/365.25, dat$time/365.25)

#Performing some exploratory data analysis.
#TODO: More EDA. 
head(dat)
dim(dat)
summary(dat)
#

```

```{r}
#Evaluating all of the hazard models below as specified in Rubio, Espindola and 
#Montoya (2023). 
#Not going to make too many in-depth comments as we are focused on the 
#implementation of the Hessian method, but there will be sufficient comments
#to explain what is going on. 
#See Rubio (2023) for more details, but the main distribution we are looking at
#is the PGW (power-generalised Weibull) model, where the "random variable"
#is mainly defined by a linear predictor, in particular, we have the structure
#h(t; alpha, beta, xi) = h_0(t*exp{x^~'_i%*%alpha}, eta)exp{x'_i%*%beta}})
#which defines the General Hazard (GH) structure. We optimise (OPT) over the GH
#structure, for the cases alpha = 0 (Proportional hazards, PH), alpha = beta, 
#x^~'_i = x'_i (Accelerated Failure Time, AFT), and the case where we have NO
#covariates at all, i.e. alpha = beta = 0 (0). 

#This is NOT necessary to understand for our purposes - the only thing we need 
#is a single case as we just want to test the Hessian method, so 
#we use the numerically optimised results for the GH case with the
#PGW distribution in further steps, OPTPGWGH. 

#To perform this optimisation, we use the GHMLE() function from the HazReg 
#package, which evaluates the MLE for a model with a GH structure. 
#I will not explain the inputs as that will bulk up the comments too much, 
#but one can use ?GHMLE to understand the inputs more. 
OPTPGW0 = GHMLE(init = rep(0, 3), times = times, status = status, 
                hstr = "baseline", dist = "PGW", method = "nlminb", maxit = 1E4)

OPTPGWPH = GHMLE(init = c(OPTPGW0$OPT$par*0, rep(0, ncol(x))), times = times, 
                status = status, hstr = "PH", dist = "PGW", des = x, 
                method = "nlminb", maxit = 1E4)

OPTPGWAFT = GHMLE(init = c(OPTPGW0$OPT$par, rep(0, ncol(x))), times = times, 
                status = status, hstr = "AFT", dist = "PGW", des = x, 
                method = "nlminb", maxit = 1E4)

OPTPGWGH = GHMLE(init = c(OPTPGW0$OPT$par, rep(0, ncol(xt)), rep(0,ncol(x))), 
                 times = times, status = status, hstr = "GH", dist = "PGW", 
                 des = x, des_t = xt, method = "nlminb", maxit = 1E4)

#Defining ALL of their MLEs as well, as this was used to ensure that 
#the results aligned with those of Rubio, Montoya and Espinola (2023).
#Furthermore, we rounded them to 3 d.p. to ensure the results agreed with those 
#in the paper, using the round(, digits = 3) function. 
#To get the MLEs we note that a log-link was used for the first 3 parameters 
#(see ?GHLME - arugment init for more details), so their MLEs are exponentiated, 
#while the other MLEs are left as is.
MLE.PGW0 = exp(OPTPGW0$OPT$par[1:3])
round(MLE.PGW0, digits = 3)
MLE.PGWPH = c(exp(OPTPGWPH$OPT$par[1:3]), OPTPGWPH$OPT$par[-c(1:3)])
round(MLE.PGWPH, digits = 3)
MLE.PGWAFT = c(exp(OPTPGWAFT$OPT$par[1:3]), OPTPGWAFT$OPT$par[-c(1:3)])
round(MLE.PGWAFT, digits = 3)
MLE.PGWGH = c(exp(OPTPGWGH$OPT$par[1:3]), OPTPGWGH$OPT$par[-c(1:3)])
round(MLE.PGWGH, digits = 3)
```

```{r, eval = F}
#This snippet of code is not necessary for our purposes and was included as a 
#safety check, to ensure that everything above was done correctly. 
#Evaluating their AICs. 
AIC.PGW0 = 2*OPTPGW0$OPT$objective + 2*length(MLE.PGW0)
AIC.PGWGH = 2*OPTPGWGH$OPT$objective + 2*length(MLE.PGWGH)
AIC.PGWAFT = 2*OPTPGWAFT$OPT$objective + 2*length(MLE.PGWAFT)
AIC.PGWPH = 2*OPTPGWPH$OPT$objective + 2*length(MLE.PGWPH)

#Ranking the AICs. 
#We first store all of them in the vector AICs.
AICs = c(AIC.PGW0, AIC.PGWPH, AIC.PGWAFT, AIC.PGWGH)
#Assigning names to them corresponding to their actual names. 
names(AICs) = c("AIC.PGW0", "AIC.PGWPH", "AIC.PGWAFT", "AIC.PGWGH")
#Using the which.min() command to identify which AIC is the lowest. 
names(AICs)[which.min(AICs)]
#Interestingly, it is the more specific PGWPH model, vs. the PGWGH model.  
which.min(AICs)
AICs
```

## Near redundancy via the Hessian Method
```{r}
#This section describes the implementation of the Hessian method onto the 
#PGW-GH model, and is complementary to chapter (insert number here). 

#Following the algorithm defined in Cole (2020) and outlined within chapter 
#(insert number here) in the dissertation, we start by evaluating the Hessian 
#numerically using the hessian() function, from the numDeriv package. 
hess.pgw = -hessian(func = OPTPGWGH$log_lik, x = OPTPGWGH$OPT$par)
#Getting the eigenvalues of the numerical hessian matrix using the eigen() 
#command, and storing them in the vector eigen.val.
eigen.val = eigen(hess.pgw)$values
#As required, taking the absolute value of the eigenvalues divided by the 
#largest eigenvalue as reference values, and storing them in the vector ref.val.
ref.val = abs(as.vector(eigen.val))/max(abs(as.vector(eigen.val)))
#Sorting our reference values in descending order and storing this in the vector
#sev. 
sev = sort(ref.val)
sev

#We see here that the largest reference value is smaller than 0.001, so there
#COULD be practical identifiability issues here with regards to the model...
#But what kind? 
sev[1] < 1E-3

```
## Bootstrapping
```{r}
#This section is dedicated to evaluating confidence intervals using the 
#Bootstrap, as the Hessian for non-identifiable models is singular. 
#Hence, it is necessary to utilise other methods, such as the Bootstrap. 

#Defining the number of bootstrap samples, B, to be 1000. 
B = 1E3

#Defining an empty vector, MLE.B, which will contain all of the bootstrapped
#MLEs for all B = 1000 samples.
MLE.B = matrix(0, ncol = length(MLE.PGWGH), nrow = B)
#Defining a completely empty vector indHess, which will show the result of 
#performing the above 
indHess = vector()
for (i in 1:B){
  #Deciding which observation (i.e. index) from dat we will use to bootstrap,
  #and using sample() to get the index to be bootstrapped over, ind.
  ind = sample(1:n, replace = T)
  #Using GHMLE() again to fit the model, just like for OPTPGWGH, except 
  #only considering the inputs corresponding to observatoin ind.
  OPTB = GHMLE(init = c(rep(0, ncol(xt) + 3 + ncol(x))), times = times[ind], 
               status = status[ind], hstr = "GH", dist = "PGW", des = x[ind,], 
               des_t = xt[ind,], method = "nlminb", maxit = 1E4)
  MLE.B[i, ] = c(exp(OPTB$OPT$par[1:3]), OPTB$OPT$par[-c(1:3)])
  #The following code is just the implementation from above of the Hessian 
  #method - to avoid bloat, please refer to the comments from the preceeding
  #section., 
  #This was originally a bug, where it evaluated the hessian using the wrong 
  #optimised output - I found it, and this is mentioned in the corresponding 
  #RPub (https://rpubs.com/FJRubio/NRPNILung) as a disclaimer.
  hessb = -hessian(OPTB$log_lik, x = OPTB$OPT$par)
  eigen.val = eigen(hessb)$values
  ref.val = abs(as.vector(eigen.val))/abs(max(as.vector(eigen.val)))
  sev = sort(ref.val)
  indHess[i] = as.numeric(sev[1]<0.001)
}

#Finding the mean of the indHess vector to get the sample proportion/empirical
#probability of getting a near-redundant model. 
mean(indHess)
#Use MLE.B to make confidence region??? 
#Marginalising? 
par1.CI = quantile(x = MLE.B[,1], c(0.025, 0.975))
#TODO: find some sort of summary statistic here...
#done - just use the Bootstrapped-confidence intervals. 

```

## Practical Non-identifiability 

```{r}
#MODIFICATION - removed p0, p1 as they were simply not necessary. 

#As in example 1, we define p to be the number of parameters, and ML to be the 
#likelihood evaluated at the MLE. 
p = length(MLE.PGWGH)
ML = OPTPGWGH$OPT$objective
#Carry-over from my modified prof.lik function in example 1. 
#Note here, compared to example 1, we changed the optimisation object to 
#OPTPGWGH. 
prof.lik = function(parint, ind, init = "zero", max.iter = 1E4, 
                    method = "nlminb"){
  #Creating the tempf function to perform 
  tempf = function(par){
    tempv = rep(0, p)
    tempv = replace(x = tempv, c(1:p)[-ind], par)
    tempv[ind] = parint
    return(OPTPGWGH$log_lik(tempv))
  }
  #MODIFICATION - condensed the two conditional statements into a single 
  #conditional using truthy values & their correspondence to 0/1.
  #MODIFICATION - Add the max.iter variable here
  if (method == "nlminb"){
    #Evaluate the output out as the negative of the nlimb() optimisation output, 
    #since this will mean we get a maximum, as nlminb() finds minima. We then 
    #also add on the 
    out = -nlminb(OPTPGWGH$OPT$par[-ind]*rep(init == "MLE", 
                  length(OPTPGWGH$OPT$par[-ind])), 
                  tempf, control = list(iter.max = max.iter))$objective + ML
  }
  #MODIFICATION - Add a conditional statement for any non-nlminb method 
  #supported in optim. 
  else{
    #Evaluate the output out as the negative of the optim() minimisation output
    #to get a maximum estimate.
    out = -optim(par = OPTPGWGH$OPT$par[-ind]*rep(init == "MLE", 
      length(OPTPGWGH$OPT$par[-ind])), fn = tempf, method = method, control = 
      list(maxit = max.iter, reltol = 1E-5))$value + ML
  }
  #We return the exponential of the output since we used the log-likelihood.
  #This returns the profile likelihood. 
  #Uniquely here, we add an additional check, which sees is the exponential of 
  #the optimised outcome is less than or equal to one, and if 
  return(rep(exp(out) <= 1, length(exp(out)))*exp(out))
}

#We evaluate the profile likelihoods for all 7 parameters of this model, prof1 
#to prof7. Recall that the first 3 parameters, the ones which define the shape 
#and scale (as seem from ?GHMLE), are log-linked, so we need to invert this. 
#To evaluate the profile likelihood, we use a combination of the above prof.lik
#command and the Vectorize() command to create an object which can procedurally
#be evaluated within the curve() command for any number of steps, n. 
prof1 = Vectorize(function(par) prof.lik(log(par),1))
curve(prof1,0.01,7 , n = 500, lwd = 2, xlab = expression(sigma), ylab = "Profile Likelihood")
prof2 = Vectorize(function(par) prof.lik(log(par),2))
curve(prof2,0.8,2 , n = 500, lwd = 2, xlab = expression(nu), ylab = "Profile Likelihood")
prof3 = Vectorize(function(par) prof.lik(log(par),3))
curve(prof3,0.25,2.5 , n = 200, lwd = 2, xlab = expression(gamma), ylab = "Profile Likelihood")
prof4 = Vectorize(function(par) prof.lik(par,4))
curve(prof4,-5,5, n = 500, lwd = 2, xlab = expression(alpha[1]), ylab = "Profile Likelihood", ylim = c(0,1))
prof5 = Vectorize(function(par) prof.lik(par,5))
curve(prof5,-2,2 , n = 200, lwd = 2, xlab = expression(beta[1]), ylab = "Profile Likelihood", ylim = c(0,1))
prof6 = Vectorize(function(par) prof.lik(par,6))
curve(prof6,-1.1, -0.1 , n = 200, lwd = 2, xlab = expression(beta[2]), ylab = "Profile Likelihood")
prof7 = Vectorize(function(par) prof.lik(par,7))
curve(prof7,0,0.65, n = 200, lwd = 2, xlab = expression(beta[3]), ylab = "Profile Likelihood")
detach(lung)

```