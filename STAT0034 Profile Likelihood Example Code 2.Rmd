---
title: "Example 2"
output: html_document
date: "2024-06-02"
bibliography: "references2.bib"
csl: ucl-institute-of-education-harvard.csl
nocite: |
  @*
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Formulation of the Tobit-2 log-likelihood (Ch. 2)

```{r}
#The following implementation was taken from Rubio (2024).
#ADDITION: setting the seed for replicability. 
set.seed(1111)
#Importing the sampleSelection (Toomet and Henningsen (2011)) package - This is the focal package of my dissertation. 

library(sampleSelection)

#Importing the libraries ssmrob(de Souza Bastos and Barreto de Souza (2022)) and ssmodels(Zhelonkin and Ronchetti (2021)), which are for robust inference and 
#modelling for sample selection models.

library(ssmrob)
library(ssmodels)

#Importing numDeriv to apply the Hessian method, for curiosity 
#(Gilbert and Varadhan (2019)). 
library(numDeriv)

#Defining the MLE for Tobit-2 model as the function MLETobit2, as discussed in 
#Chapter 2 of my dissertation, from Toomet and Henningsen (2008), modified 
#following Rubio(2024a)

#This takes inputs init, the inital value from where the parameter estimate is 
#evaluated from, out, the responses of the observable model, sel, the responses 
#of the selection model, deso, the design matrix for the observable model, less 
#the first column, dess, the design matrix for the selection process less the 
#first column, method, the optimisation method, and maxit, the maximum number of 
#iterations, with default number 100.

MLETobit2 = function(init, out, sel, deso, dess, beta_par = 0,
                     method = "nlminb", maxit = 100){
  #Converting the inputs out and sel into vectors.
  out = as.vector(out); sel = as.vector(sel)
  #Defining x1 and w1 as the design matrices of the observable and selection 
  #processes respectively by adding back the missing column of 1's.
  x1 = as.matrix(cbind(1, deso)); w1 = as.matrix(cbind(1, dess))
  #Defining p and q as the number of columns of x1 and w1 respectively. 
  p = ncol(x1); q = ncol(w1)
  #Defining a function which evaluates the log-likelihood of the Tobit-2 model,
  #loglik. This takes in as inputs a parameter vector, par. 
  loglik = function(par){
    #Reparametrising sigma and rho in our model, using an exponential and a 
    #tanh link. 
    sigma = exp(par[1]); rho = tanh(par[2]) 
    #Defining our linear regression coefficients for the observable model, beta, 
    #and that for the selection model, alpha. (Modified from Rubio(2023)).
    #TODO: Reparametrise beta = beta.tilde
    beta = par[c(3:(p+2))]; alpha = par[(p+3):(p+q+2)]
    #Reparametrisation if beta_par = 1
    if(beta_par){
      beta = beta/sigma
    }
    #Defining the linear predictors of the observable process (x_beta) and the 
    #the selection process (w_alpha). 
    x_beta = x1%*%beta; w_alpha = w1%*%alpha
    #Defining inds0 and inda1 as indicators corresponding to the selection 
    #criterion, which can be seen in chapter 2. 
    inds0 = (sel == 0); inds1 = (sel == 1)
    #Evaluating the log-likelihood of the Tobit-2 model as given in Chapter 2. 
    ll = sum(pnorm(-w_alpha[inds0], log = T)) + 
         sum(pnorm( (w_alpha[inds1] + rho*(out[inds1] - x_beta[inds1])/sigma)/
                      sqrt(1 - rho^2), log = T)) + 
    #MODIFICATION: compressed the explicit evaluation of the normal component of
    #the observable processes into the dnorm function, as the distribution is a
    #N(x_beta, sigma^2) distributed random variable from proof (INSERT). 
         sum(dnorm(out[inds1], mean = x_beta[inds1], sd = sigma, log = T))
    #Returning the log-loss, which we will minimise using the optim commands. 
    #This is because optim and nlminb are principally minimisation algorithms, 
    #so returning the log-loss is more efficient here. 
    return(-ll)
  }
  #Conditional to evaluate the optimal value using nlminb if specified
  #(by default).
  if(method == "nlminb"){ 
    OPT = nlminb(init, loglik, control = list(iter.max = maxit))
  }
  #MODIFICATION- Conditional to evaluate the optimal value using the optim() 
  #function
  else{
    #Using optim to allow us to select our desired optimisation algorithm.
    OPT = optim(init, loglik, control = list(maxit = maxit), method = method)
  }
  #Storing the MLES in a vector, remembering to undo the links defined above. 
  MLE = c(exp(OPT$par[1]), tanh(OPT$par[2]), OPT$par[-c(1,2)])
  #Creating a vector of estimate names.
  names(MLE) = c("sigma", "rho", "intercept", paste(paste("beta_hat[,", 1:(p-1), 
    sep = "" ),"]", sep = ""), "intercept", paste(paste("alpha_hat[,", 1:(q-1), 
    sep = "" ),"]", sep = "")) 
  #Storing all our outputs in the list outf.
  outf = list(loglik = loglik, OPT = OPT, MLE = MLE)
  #Returning outf as the function output. 
  return(outf)
  
}
```

## Profile Likelihood, applied onto MEPS2001

```{r MEPS2001, echo=FALSE}
#Attach the MEPS2001 dataset from the ssmrob() package 
#(Zhelokin and Ronchetti (2021)). 
#The following analysis follows Rubio (2024a)
attach(MEPS2001)

#Beginning of exploratory data analysis. 
#?MEPS2001 - Read documentation for this dataset.
#Look at the top few elements for eDA.
head(MEPS2001)

#Exploratory data analysis. 

#Defining design matrices (less the intercept column). 
#... for the observable model.

par(mfrow = c(1, 1))

dm.obs = cbind(age, female, educ, blhisp, totchr, ins)
#Taking a boxplot of age, educ, and totchr, as these are numeric. 
boxplot(dm.obs[, c(1, 3, 5)], xlab = "Covariate", 
        col = c("red", "blue", "gray"), main = "Observable Process")
legend(x = "topright", legend = c("Age (Years)", "Education (Years)", 
                                  "# of Chronic Diseases"), 
       col = c("red", "blue", "gray"), pch = rep(16, 3))
summary(dm.obs)

#Taking the sample proportion of female, blhisp and ins with colMeans().
colMeans(dm.obs[,c(2, 4, 6)])

#... for the selection model.
dm.sel = cbind(dm.obs, income)
#Taking a boxplot of income, as both processes only differ by income. 
boxplot(dm.sel[, 7],xlab = "Income", ylab = "Income($USD '000)", 
        col = "brown", main = "Selection Process")
summary(dm.sel)

#Selection response, a binary (0/1) variable which acts as the indicator 
#function corresponding to the latent variable for positive ambulatory 
#expenditures. 

sel = as.logical(dambexp)

#Taking the sample proportion with mean().  
mean(sel)

#observable responses, corresponding to log ambulatory expenditures, lnambx.
y = lnambx

#Finding the MLE of the Tobit-2 likelihood, with unscaled parameter. 
OPT = MLETobit2(init = rep(0, 17), out = y, sel = sel, deso = dm.obs, 
                dess = dm.sel, method = "nlminb", maxit = 1E4, beta_par = 0)

#Getting and printing the MLEs for the Tobit-2 likelihood, for this specific
#dataset. 
MLE = OPT$MLE
MLE

#Getting the log-likelihood, which is the negative of the log-loss, from the  
#MLETobit2() function. 
-OPT$OPT$objective

#Doing the same thing, but with scaled beta parameter.
OPT2 = MLETobit2(init = rep(0, 17), out = y, sel = sel, deso = dm.obs, 
                dess = dm.sel, method = "nlminb", maxit = 1E4, beta_par = 1)
MLE2 = OPT2$MLE
MLE2

#Getting the base-10 log of the difference of the two MLEs. The biggest 
#difference seems to be that of the intercept, 
log(abs(MLE-MLE2), 10)

#Comparing this to other optimisation algorithms. 
#Defining two equation objects, selectEq and obsEq, which define the 
#relationship between the response and the covariates for the selection and
#observable process respectively. 
selectEq = dambexp ~ age + female + educ + blhisp + totchr + ins + income
obsEq = update(selectEq, . - dambexp + y ~ . - income)

#Using the HeckmanCL function from the ssmodels package to perform MLE on the 
#data, repeating what we did above with MLETobit2, and storing the output in 
#HMLE.
HMLE = HeckmanCL(selectEq, obsEq, data = MEPS2001)
#Retrieving a summary for HMLE.
coef(HMLE)

#Repeating the same analysis as above, but with the selection() function from 
#sampleSelection, and storing the output in ssMLE. 
ssMLE = selection(selectEq, obsEq)
summary(ssMLE)

#Repeating the same analysis as above, this time with the 2-step estimator. 
ssMLE.2step = selection(selectEq, obsEq, data = MEPS2001, method = "2step")
coef(ssMLE.2step)
```

## Applying the profile likelihood
```{r}
#Defining p, the number of parameters in the model, and ML, the log-loss of 
#the original MLETobit2 model. 
p = length(MLE)
ML = OPT$OPT$objective

#Create the function prof.lik to evaluate the profile likelihood MLE. 
#This has parameters parint, for the parameter of interest (or its
#reparametrisations), ind, the corresponding index, init, the initial value used
#for optimisation, either from zeros or from the regular MLE, max.iter, the 
#number of iterations, and method, the optimisation algorithm used.
#MODIFICATIONS: Added method and max.iter as new parameters. 
prof.lik = function(parint, ind, init = "zero", max.iter = 1E4, method = "nlminb"){
  #Creating the tempf function as a dummy function to perform optimisation over
  #the log-likelihood.
  tempf = function(par){
    tempv = rep(0, p)
    tempv = replace(x = tempv, c(1:p)[-ind], par)
    tempv[ind] = parint
    return(OPT$loglik(tempv))
  }
  #MODIFICATION - condensed the two conditional statements into a single 
  #conditional using truthy values & their correspondence to 0/1.
  #MODIFICATION - Add the max.iter variable here
  if (method == "nlminb"){
    #Evaluate the output out as the negative of the nlimb() optimisation output, 
    #since this will mean we get a maximum, as nlminb() finds minima. 
    out = -nlminb(OPT$OPT$par[-ind]*rep(init == "MLE", 
            length(OPT$OPT$par[-ind])), tempf, control = list(iter.max =
            max.iter))$objective + ML
  }
  #MODIFICATION - Add a conditional statement for any non-nlminb method 
  #supported in optim. 
  else{
    #Evaluate the output out as the negative of the optim() minimisation output
    #to get a maximum estimate.
    out = -optim(par = OPT$OPT$par[-ind]*rep(init == "MLE", 
      length(OPT$OPT$par[-ind])), fn = tempf, method = method, control = 
      list(maxit = max.iter, reltol = 1E-5))$value + ML
  }
  #We return the exponential of the output since we used the log-likelihood.
  #This returns the profile likelihood. 
  return(exp(out))
}

#Interpreted from ?Vectorize call - create a vector-function hybrid object where 
#if you pass it vector values it will output a vector of corresponding function
#outputs. In this case, one can imagine here that it creates a vector object for 
#our previously defined prof.lik function, and when we pass a vector to it, it 
#should create a vector of the outputs of the prof.lik() function for all 
#elements in the vector. 

#Recall here we used an exponential link for sigma, the first parameter, so we
#revert it here. 
prof1 = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, 
                                         init = "zero"))

#Plots the profile likelihood using the curve() function. 
#NOTE: This takes a long time to run. 
curve(prof1, 1.2, 1.35, n = 50, lwd = 1, xlab = expression(sigma), 
      ylab = "Profile Likelihood", main = "zero-nlminb")
#Adding a line corresponding to the MLE.
abline(v = OPT$MLE[1], col = "red", lwd = 2)
#Adding a legend for clarity. 
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")
#par(mfrow = c(1, 2))

#Perform the same procedure from above, but start with the MLE as an initial 
#value. 
prof1a = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, 
  init = "MLE", method = "nlminb"))

curve(prof1a, 1.2, 1.35, n = 50, lwd = 1, xlab = expression(sigma), 
      ylab = "Profile Likelihood", col = "brown", main = "MLE-nlminb") 
abline(v = OPT$MLE[1], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")
#Below commented out to streamline code (no change in results). 
#Perform the same procedure as for prof1a, but this time using 100k iterations
#instead of 10k to diagnose the roughness of the MLE profile likelihood curve. 
#prof1b = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, 
#  init = "zero", max.iter = 1E5, method = "BFGS"))

#curve(prof1b, 1.2, 1.35, n = 50, lwd = 2, xlab = expression(sigma), 
#      ylab = "Profile Likelihood")

#Redoing prof1a, except using the "BFGS" algorithm instead of the nlminb method.
prof1c = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, 
  init = "MLE", method = "BFGS"))
curve(prof1c, 1.2, 1.35, n = 50, lwd = 1, xlab = expression(sigma), 
      ylab = "Profile Likelihood", col = "blue", main = "MLE-BFGS")
abline(v = OPT$MLE[1], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")
#par(mfrow = c(1, 1))

#Performing the same procedure as in prof1a, except this time for rho.
#Note that as we used a tanh link for the rho parameter, we need to invert the
#transformation when plotting the profile likelihood.

#par(mfrow = c(2,2))
prof2 = Vectorize(function(par) prof.lik(parint = atanh(par), ind = 2, 
                                    init = "MLE", method = "nlminb"))
curve(prof2,-0.6, 0.25 , n = 50, lwd = 1, xlab = expression(rho), 
      ylab = "Profile Likelihood"
      )
abline(v = OPT$MLE[2], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")
#Performing the same procedure here as well for beta_int, the intercept term for 
#the observable process. 
prof3 = Vectorize(function(par) prof.lik(parint = par, ind = 3, init = "MLE"))
curve(prof3, 4.25, 6, n = 50, lwd = 1, xlab = expression(beta["int"]), 
      ylab = "Profile Likelihood"
      )
abline(v = OPT$MLE[3], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

#Performing the same procedure for all other parameters in the model. 
#WARNING: Each graph takes a long time to plot. Execute at your own risk. 
prof4 = Vectorize(function(par) prof.lik(parint = par, ind = 4, init = "MLE"))
curve(prof4, 0.15, 0.26, n = 50, lwd = 1, xlab = expression(beta["age"]), 
      ylab = "Profile Likelihood"
      )
abline(v = OPT$MLE[4], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof5 = Vectorize(function(par) prof.lik(parint = par, ind = 5, init = "MLE"))
curve(prof5, 0.2, 0.5 , n = 50, lwd = 1, xlab = expression(beta["female"]), 
      ylab = "Profile Likelihood"
      )
abline(v = OPT$MLE[5], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof6 = Vectorize(function(par) prof.lik(parint = par, ind = 6, init = "MLE"))
curve(prof6, -0.04, 0.08 , n = 50, lwd = 1, xlab = expression(beta["educ"]), 
      ylab = "Profile Likelihood"
      )
abline(v = OPT$MLE[6], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof7= Vectorize(function(par) prof.lik(parint = par, ind = 7, init = "MLE"))
curve(prof7, -0.5,0.1 , n = 50, lwd = 1, xlab = expression(beta["blhisp"]), 
      ylab = "Profile Likelihood"
      )
abline(v = OPT$MLE[7], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof8 = Vectorize(function(par) prof.lik(parint = par, ind = 8, init = "MLE"))
curve(prof8, 0.44, 0.64 , n = 50, lwd = 1, xlab = expression(beta["totchr"]), 
      ylab = "Profile Likelihood"
      )
abline(v = OPT$MLE[8], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof9 = Vectorize(function(par) prof.lik(parint = par, ind = 9, init = "MLE"))
curve(prof9, -0.15, 0.15 , n = 50, lwd = 1, xlab = expression(beta["ins"]), 
      ylab = "Profile Likelihood"
      )
abline(v = OPT$MLE[9], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")
  
prof10 = Vectorize(function(par) prof.lik(parint = par, ind = 10, init = "MLE"))
curve(prof10, -1.5, 0.5 , n = 50, lwd = 1, xlab = expression(alpha["int"]), 
      ylab = "Profile Likelihood"
      )
abline(v = OPT$MLE[10], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof11 = Vectorize(function(par) prof.lik(parint = par, ind = 11, init = "MLE"))
curve(prof11, 0, 0.18, n = 50, lwd = 1, xlab = expression(alpha["age"]), 
      ylab = "Profile Likelihood"
      )
abline(v = OPT$MLE[11], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof12 = Vectorize(function(par) prof.lik(parint = par, ind = 12, init = "MLE"))
curve(prof12, 0.2, 1 , n = 50, lwd = 1, xlab = expression(alpha["female"]), 
      ylab = "Profile Likelihood"
      )
abline(v = OPT$MLE[12], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof13 = Vectorize(function(par) prof.lik(parint = par, ind = 13, init = "MLE"))
curve(prof13, 0, 0.2, n = 50, lwd = 1, xlab = expression(alpha["educ"]), 
      ylab = "Profile Likelihood"
      )
abline(v = OPT$MLE[13], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof14 = Vectorize(function(par) prof.lik(parint = par, ind = 14, init = "MLE"))
curve(prof14, -0.6, -0.1 , n = 50, lwd = 1, xlab = expression(alpha["blhisp"]), 
      ylab = "Profile Likelihood"
      )
abline(v = OPT$MLE[14], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof15 = Vectorize(function(par) prof.lik(parint = par, ind = 15, init = "MLE"))
curve(prof15, 0.4, 1.2, n = 50, lwd = 1, xlab = expression(alpha["totchr"]), 
      ylab = "Profile Likelihood"
      )
abline(v = OPT$MLE[15], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof16 = Vectorize(function(par) prof.lik(parint = par, ind = 16, init = "MLE"))
curve(prof16, -0.2, 0.4, n = 50, lwd = 1, xlab = expression(alpha["ins"]), 
      ylab = "Profile Likelihood"
      )
abline(v = OPT$MLE[16], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

prof17 = Vectorize(function(par) prof.lik(parint = par, ind = 17, init = "MLE"))
curve(prof17, -0.01, 0.01, n = 50, lwd = 1, xlab = expression(alpha["income"]), 
      ylab = "Profile Likelihood"
      )
abline(v = OPT$MLE[17], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

#par(mfrow = c(1,1))

```

### Constructing a Profile Likelihood Confidence Interval 
```{r}
#Inspired by the implementation from Rubio (2024b). 
#MODIFICATION - Changed rho to sigma
#MODIFICATION - added the variable bound to denote the 0.147 boundary point
#for our profile likelihood confidence interval.
#Defining our significance level, SL = 0.05
SL = 0.05
#Defining bound following Lemma 4.1 in my dissertation. 
bound = exp(-0.5*qchisq(1-SL, df = 1))
################################################################################
#Vectorise a shifted version of the sigma parameter (parameter 2), to create a
#0.147-profile likelihood confidence interval - see Chapter 4 for more details.
intprof1 = Vectorize(function(sigma) prof1(sigma) - bound)
#Plotting this new profile log-likelihood with the curve() function.
curve(prof1, 1.20,1.35, n = 50, lwd = 1, xlab = expression(sigma), ylab = 
        "Profile Likelihood")
#... then adding the line corresponding to 0.147 to show the confidence interval
#of interest. 
abline(h = bound, lwd = 2, col = "blue")
abline(v = OPT$MLE[1], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")

#Evaluating the lower and upper profile likelihood confidence interval, LP and
#UP respectively, for a 0.147-confidence interval for sigma, using uniroot()

LP = uniroot(f = intprof1, interval = c(1.20,1.25))$root
UP = uniroot(f = intprof1, interval = c(1.30,1.35))$root

profint1 = exp(c(LP, UP))

#Repeating this with all other parameters (without plotting - that would be too
#cumbersome)

#NOTE: For rho, we take the implementation from Rubio (2023)
intprof2 = Vectorize(function(rho) prof2(rho) - bound)
LP2 = uniroot(f = intprof2, interval = c(-0.5, -0.4))$root
UP2 = uniroot(f = intprof2, interval = c(0.1, 0.2))$root

profint2 = tanh(c(LP2, UP2))
#For the rest, we create 2 vectors; LPvec, UPvec, to store all the interval 
#bounds. This will allow us to iteratively assign the confidence intervals all 
#at once.
LPvec = rep(NA, p-2)
UPvec = rep(NA, p-2)

intprof3 = Vectorize(function(beta) prof3(beta) - bound)
LPvec[1] = uniroot(f = intprof3, interval = c(4.5, 5))$root
UPvec[1] = uniroot(f = intprof3, interval = c(5.1, 5.7))$root

intprof4 = Vectorize(function(beta) prof4(beta) - bound)
LPvec[2] = uniroot(f = intprof4, interval = c(0.16, 0.18))$root
UPvec[2] = uniroot(f = intprof4, interval = c(0.24, 0.26))$root

intprof5 = Vectorize(function(beta) prof5(beta) - bound)
LPvec[3] = uniroot(f = intprof5, interval = c(0.2, 0.25))$root
UPvec[3] = uniroot(f = intprof5, interval = c(0.43, 0.5))$root

intprof6 = Vectorize(function(beta) prof6(beta) - bound)
LPvec[4] = uniroot(f = intprof6, interval = c(-0.01, 0.01))$root
UPvec[4] = uniroot(f = intprof6, interval = c(0.03, 0.05))$root

intprof7 = Vectorize(function(beta) prof7(beta) - bound)
LPvec[5] = uniroot(f = intprof7, interval = c(-0.5, -0.3))$root
UPvec[5] = uniroot(f = intprof7, interval = c(-0.15,0))$root

intprof8 = Vectorize(function(beta) prof8(beta) - bound)
LPvec[6] = uniroot(f = intprof8, interval = c(0.4, 0.48))$root
UPvec[6] = uniroot(f = intprof8, interval = c(0.57, 0.62))$root

intprof9 = Vectorize(function(beta) prof9(beta) - bound)
LPvec[7] = uniroot(f = intprof9, interval = c(-0.18, -0.1))$root
UPvec[7] = uniroot(f = intprof9, interval = c(0.05, 1))$root

intprof10 = Vectorize(function(alpha) prof10(alpha) - bound)
LPvec[8] = uniroot(f = intprof10, interval = c(-1.2, -0.8))$root
UPvec[8] = uniroot(f = intprof10, interval = c(-0.4, -0.1))$root

intprof11 = Vectorize(function(alpha) prof11(alpha) - bound)
LPvec[9] = uniroot(f = intprof11, interval = c(0, 0.045))$root
UPvec[9] = uniroot(f = intprof11, interval = c(0.12, 0.16))$root

intprof12 = Vectorize(function(alpha) prof12(alpha) - bound)
LPvec[10] = uniroot(f = intprof12, interval = c(0.45, 0.6))$root
UPvec[10] = uniroot(f = intprof12, interval = c(0.72, 0.8))$root

intprof13 = Vectorize(function(alpha) prof13(alpha) - bound)
LPvec[11] = uniroot(f = intprof13, interval = c(0.02, 0.045))$root
UPvec[11] = uniroot(f = intprof13, interval = c(0.06, 0.1))$root

intprof14 = Vectorize(function(alpha) prof14(alpha) - bound)
LPvec[12] = uniroot(f = intprof14, interval = c(-0.55, -0.45))$root
UPvec[12] = uniroot(f = intprof14, interval = c(-0.28, -0.22))$root

intprof15 = Vectorize(function(alpha) prof15(alpha) - bound)
LPvec[13] = uniroot(f = intprof15, interval = c(0.6, 0.7))$root
UPvec[13] = uniroot(f = intprof15, interval = c(0.9, 1))$root

intprof16 = Vectorize(function(alpha) prof16(alpha) - bound)
LPvec[14] = uniroot(f = intprof16, interval = c(0, 0.05))$root
UPvec[14] = uniroot(f = intprof16, interval = c(0.28, 0.34))$root

intprof17 = Vectorize(function(alpha) prof17(alpha) - bound)
LPvec[15] = uniroot(f = intprof17, interval = c(0, 0.0015))$root
UPvec[15] = uniroot(f = intprof17, interval = c(0.004, 0.006))$root

#Using a for loop to get all of the confidence intervals desired, iterating over
#the length of UPvec, and using assign() and paste(). 

for (i in 1:length(UPvec)){
  assign(paste("profint", i+2, sep = ""), value = c(LPvec[i], UPvec[i]))
}

```

### Hessian Method
```{r}
#THIS IS A NEW ADDITION TO THE CODE - NOT FOUND IN RUBIO (2024). 
#Performing the procedure as described in Cole (2020), pp. 72-73.
#Setting seed again, just in case. 
set.seed(1111)
#Evaluating the Hessian at the numerically-obtained MLE. 
hess.model = -hessian(func = OPT$loglik, x = OPT$OPT$par)
#Getting the Hessian's eigenvalues.
eigen.vals = eigen(hess.model)$values
#Getting the reference values as defined in Cole (2020), p.94 
#and in Chapter 5 of the Dissertation.
ref.vals = abs(as.vector(eigen.vals))/max(abs(as.vector(eigen.vals)))
#Getting sev as the sorted vector of reference values. 
sev = sort(ref.vals)
sev
#Checking if the larges reference value is lower than the 1E-9 threshold for 
#non-identifiability. 
sev[1] < 1E-9
```