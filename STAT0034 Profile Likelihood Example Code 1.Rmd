---
title: "Example 1 - Tobit-2 Model"
output: html_document
date: "2024-06-02"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Formulation of the Tobit-2 log-likelihood (Ch. 2)

```{r}
#The following implementation was taken from Rubio (2024).
#ADDITION: setting the seed for replicability. 
set.seed(1111)
#Importing the sampleSelection package - This is the focal package of my 
#dissertation. 

library(sampleSelection)

#Importing the libraries ssmrob and ssmodels, which are for robust inference and 
#modelling for sample selection models.

library(ssmrob)
library(ssmodels)

#Importing libraries from Rubio, Montoya and Espinola (2024) to apply the 
#Hessian method, for curiosity. 
library(numDeriv)

#Defining the MLE for Tobit-2 model as the function MLETobit2, as discussed in 
#Chapter 2 of my dissertation.

#This takes inputs init, the inital value from where the parameter estimate is 
#evaluated from, out, the responses of the observable model, sel, the responses 
#of the selection model, deso, the design matrix for the observable model, less 
#the first column, dess, the design matrix for the selection process less the 
#first column, method, the optimisation method, and maxit, the maximum number of 
#iterations, with default number 100.

MLETobit2 = function(init, out, sel, deso, dess, beta_par = 0,
                     method = "nlminb", maxit = 100){
  #Converting the inputs out and sel into vectors.
  out = as.vector(out); sel = as.vector(sel)
  #Defining x1 and w1 as the design matrices of the observable and selection 
  #processes respectively by adding back the missing column of 1's.
  x1 = as.matrix(cbind(1, deso)); w1 = as.matrix(cbind(1, dess))
  #Defining p and q as the number of columns of x1 and w1 respectively. 
  p = ncol(x1); q = ncol(w1)
  #Defining a function which evaluates the log-likelihood of the Tobit-2 model,
  #loglik. This takes in as inputs a parameter vector, par. 
  loglik = function(par){
    #Reparametrising sigma and rho in our model, using an exponential and a 
    #tanh link. 
    sigma = exp(par[1]); rho = tanh(par[2]) 
    #Defining our linear regression coefficients for the observable model, beta, 
    #and that for the selection model, alpha. (Modified from Rubio(2023)).
    #TODO: Reparametrise beta = beta.tilde
    beta = par[c(3:(p+2))]; alpha = par[(p+3):(p+q+2)]
    #Reparametrisation if beta_par = 1
    if(beta_par){
      beta = beta/sigma
    }
    #Defining the linear predictors of the observable process (x_beta) and the 
    #the selection process (w_alpha). 
    x_beta = x1%*%beta; w_alpha = w1%*%alpha
    #Defining inds0 and inda1 as indicators corresponding to the selection 
    #criterion, which can be seen in chapter 2. 
    inds0 = (sel == 0); inds1 = (sel == 1)
    #Evaluating the log-likelihood of the Tobit-2 model as given in Chapter 2. 
    ll = sum(pnorm(-w_alpha[inds0], log = T)) + 
         sum(pnorm( (w_alpha[inds1] + rho*(out[inds1] - x_beta[inds1])/sigma)/
                      sqrt(1 - rho^2), log = T)) + 
    #MODIFICATION: compressed the explicit evaluation of the normal component of
    #the observable processes into the dnorm function, as the distribution is a
    #N(x_beta, sigma^2) distributed random variable from proof (INSERT). 
         sum(dnorm(out[inds1], mean = x_beta[inds1], sd = sigma, log = T))
    #Returning the log-loss, which we will minimise using the optim commands. 
    #This is because optim and nlminb are principally minimisation algorithms, 
    #so returning the log-loss is more efficient here. 
    return(-ll)
  }
  #Conditional to evaluate the optimal value using nlminb if specified
  #(by default).
  if(method == "nlminb"){ 
    OPT = nlminb(init, loglik, control = list(iter.max = maxit))
  }
  #MODIFICATION- Conditional to evaluate the optimal value using the optim() 
  #function
  else{
    #Using optim to allow us to select our desired optimisation algorithm.
    OPT = optim(init, loglik, control = list(maxit = maxit), method = method)
  }
  #Storing the MLES in a vector, remembering to undo the links defined above. 
  MLE = c(exp(OPT$par[1]), tanh(OPT$par[2]), OPT$par[-c(1,2)])
  #Creating a vector of estimate names...
  names(MLE) = c("sigma", "rho", "intercept", paste(paste("beta_hat[,", 1:(p-1), 
    sep = "" ),"]", sep = ""), "intercept", paste(paste("alpha_hat[,", 1:(q-1), 
    sep = "" ),"]", sep = "")) 
  #Storing all our outputs in the list outf.
  outf = list(loglik = loglik, OPT = OPT, MLE = MLE)
  #Returning outf as the function output. 
  return(outf)
  
}
```

## Profile Likelihood, applied onto MEPS2001

```{r MEPS2001, echo=FALSE}
#Attach the MEPS2001 dataset from the ssmrob package. 
attach(MEPS2001)

#Beginning of exploratory data analysis. 
#?MEPS2001 - Read documentation for this dataset.
#Look at the top of
head(MEPS2001)

#TODO: Exploratory data analysis. 

#Defining design matrices (less the intercept column). 
#... for the observable model.

par(mfrow = c(1, 1))

dm.obs = cbind(age, female, educ, blhisp, totchr, ins)
#Taking a boxplot of age, educ, and totchr, as these are numeric. 
boxplot(dm.obs[, c(1, 3, 5)], xlab = "Covariate", 
        col = c("red", "blue", "gray"))
legend(x = "topright", legend = c("age (Years)", "educ (Years)", "totchr (No.)"), 
       col = c("red", "blue", "gray"), pch = rep(16, 3))
summary(dm.obs)

#Taking the sample proportion of female, blhisp and ins with colMeans().
colMeans(dm.obs[,c(2, 4, 6)])

#... for the selection model.
dm.sel = cbind(dm.obs, income)
#Taking a boxplot of income, as both processes only differ by income. 
boxplot(dm.sel[, c(7)],xlab = "Income", ylab = "Income($USD '000)", 
        col = "brown")
summary(dm.sel)

#Selection response, a binary (0/1) variable which acts as the indicator 
#function corresponding to the latent variable for positive ambulatory 
#expenditures. 

sel = as.logical(dambexp)

#Taking the sample proportion with mean().  
mean(sel)

#observable responses, corresponding to log ambulatory expenditures, lnambx.
y = lnambx

#Finding the MLE of the Tobit-2 likelihood, with unscaled parameter. 
OPT = MLETobit2(init = rep(0, 17), out = y, sel = sel, deso = dm.obs, 
                dess = dm.sel, method = "nlminb", maxit = 1E4, beta_par = 0)

#Getting and printing the MLEs for the Tobit-2 likelihood, for this specific
#dataset. 
MLE = OPT$MLE
MLE

#Getting the log-likelihood, which is the negative of the log-loss, from the  
#MLETobit2() function. 
-OPT$OPT$objective

#Doing the same thing, but with scaled beta parameter.
OPT2 = MLETobit2(init = rep(0, 17), out = y, sel = sel, deso = dm.obs, 
                dess = dm.sel, method = "nlminb", maxit = 1E4, beta_par = 1)
MLE2 = OPT2$MLE
MLE2

#Getting the base-10 log of the difference of the two MLEs. The biggest 
#difference seems to be that of the intercept, 
log(abs(MLE-MLE2), 10)

#Comparing this to other optimisation algorithms. 
#Defining two equation objects, selectEq and obsEq, which define the 
#relationship between the response and the covariates for the selection and
#observable process respectively. 
selectEq = dambexp ~ age + female + educ + blhisp + totchr + ins + income
obsEq = update(selectEq, . - dambexp + y ~ . - income)

#Using the HeckmanCL function from the ssmodels package to perform MLE on the 
#data, repeating what we did above with MLETobit2, and storing the output in 
#HMLE.
HMLE = HeckmanCL(selectEq, obsEq, data = MEPS2001)
#Retrieving a summary for HMLE.
coef(HMLE)

#Repeating the same analysis as above, but with the selection() function from 
#sampleSelection, and storing the output in ssMLE. 
ssMLE = selection(selectEq, obsEq)
summary(ssMLE)

#Repeating the same analysis as above, this time
ssMLE.2step = selection(selectEq, obsEq, data = MEPS2001, method = "2step")
coef(ssMLE.2step)
```

## Applying the profile likelihood
```{r}
#Defining p, the number of parameters in the model, and ML, the log-loss of 
#the original MLETobit2 model. 
p = length(MLE)
ML = OPT$OPT$objective

#Create the function prof.lik to evaluate the profile likelihood MLE. 
#This has parameters parint, for the parameter of interest (or its
#reparametrisations), ind, the corresponding index, init, the initial value used
#for optimisation, either from zeros or from the regular MLE, max.iter, the 
#number of iterations, and method, the optimisation algorithm used.
#MODIFICATIONS: Added method and max.iter as new parameters. 
prof.lik = function(parint, ind, init = "zero", max.iter = 1E4, method = "nlminb"){
  #Creating the tempf function as a dummy function to perform optimisation over
  #the log-likelihood.
  tempf = function(par){
    tempv = rep(0, p)
    tempv = replace(x = tempv, c(1:p)[-ind], par)
    tempv[ind] = parint
    return(OPT$loglik(tempv))
  }
  #MODIFICATION - condensed the two conditional statements into a single 
  #conditional using truthy values & their correspondence to 0/1.
  #MODIFICATION - Add the max.iter variable here
  if (method == "nlminb"){
    #Evaluate the output out as the negative of the nlimb() optimisation output, 
    #since this will mean we get a maximum, as nlminb() finds minima. 
    out = -nlminb(OPT$OPT$par[-ind]*rep(init == "MLE", 
            length(OPT$OPT$par[-ind])), tempf, control = list(iter.max =
            max.iter))$objective + ML
  }
  #MODIFICATION - Add a conditional statement for any non-nlminb method 
  #supported in optim. 
  else{
    #Evaluate the output out as the negative of the optim() minimisation output
    #to get a maximum estimate.
    out = -optim(par = OPT$OPT$par[-ind]*rep(init == "MLE", 
      length(OPT$OPT$par[-ind])), fn = tempf, method = method, control = 
      list(maxit = max.iter, reltol = 1E-5))$value + ML
  }
  #We return the exponential of the output since we used the log-likelihood.
  #This returns the profile likelihood. 
  return(exp(out))
}

#Interpreted from ?Vectorize call - create a vector-function hybrid object where 
#if you pass it vector values it will output a vector of corresponding function
#outputs. In this case, one can imagine here that it creates a vector object for 
#our previously defined prof.lik function, and when we pass a vector to it, it 
#should create a vector of the outputs of the prof.lik() function for all 
#elements in the vector. 

#Recall here we used an exponential link for sigma, the first parameter, so we
#revert it here. 
prof1 = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, 
                                         init = "zero"))

#Plots the profile likelihood using the curve() function. 
#NOTE: This takes a long time to run. 
curve(prof1, 1.2, 1.35, n = 50, lwd = 2, xlab = expression(sigma), 
      ylab = "Profile Likelihood")

#par(mfrow = c(1, 2))

#Perform the same procedure from above, but start with the MLE as an initial 
#value. 
prof1a = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, 
  init = "MLE", method = "nlminb"))

curve(prof1a, 1.2, 1.35, n = 50, lwd = 2, xlab = expression(sigma), 
      ylab = "Profile Likelihood", col = "red", main = "MLE-nlminb") 

#Below commented out to streamline code. 
#Perform the same procedure as for prof1a, but this time using 100k iterations
#instead of 10k to diagnose the roughness of the MLE profile likelihood curve. 
#prof1b = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, 
#  init = "zero", max.iter = 1E5, method = "BFGS"))

#curve(prof1b, 1.2, 1.35, n = 50, lwd = 2, xlab = expression(sigma), 
#      ylab = "Profile Likelihood")

#Redoing prof1a, except using the "BFGS" algorithm instead of the nlminb method.
prof1c = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, 
  init = "MLE", method = "BFGS"))
curve(prof1c, 1.2, 1.35, n = 50, lwd = 1, xlab = expression(sigma), 
      ylab = "Profile Likelihood", col = "blue", main = "MLE-BFGS")

#par(mfrow = c(1, 1))

#Performing the same procedure as in prof1a, except this time for rho.
#Note that as we used a tanh link for the rho parameter, we need to invert the
#transformation when plotting the profile likelihood.
prof2 = Vectorize(function(par) prof.lik(parint = atanh(par), ind = 2, 
                                    init = "MLE", method = "nlminb"))

curve(prof2,-0.6, 0.25 , n = 50, lwd = 1, xlab = expression(atanh(rho)), 
      ylab = "Profile Likelihood", 
      main = expression(paste("Profile Likelihood for ", rho)))

#Performing the same procedure here as well for beta_0, the intercept term for 
#the observable process. 
prof3 = Vectorize(function(par) prof.lik(parint = par, ind = 3, init = "MLE"))
curve(prof3, 4.25, 6, n = 50, lwd = 1, xlab = expression(beta[0]), 
      ylab = "Profile Likelihood", 
      main = expression(paste("Profile Likelihood for ", beta[0])))

#Performing the same procedure for all other parameters in the model. 
prof4 = Vectorize(function(par) prof.lik(parint = par, ind = 4, init = "MLE"))
curve(prof4, 0.19, 0.22, n = 50, lwd = 1, xlab = expression(beta[1]), 
      ylab = "Profile Likelihood",
      main = expression(paste("Profile Likelihood for ", beta[1])))

prof5 = Vectorize(function(par) prof.lik(parint = par, ind = 5, init = "MLE"))
curve(prof5, 0.31, 0.37 , n = 50, lwd = 1, xlab = expression(beta[2]), 
      ylab = "Profile Likelihood",
      main = expression(paste("Profile Likelihood for ", beta[2])))

prof6 = Vectorize(function(par) prof.lik(parint = par, ind = 6, init = "MLE"))
curve(prof6, 0.01, 0.02 , n = 50, lwd = 1, xlab = expression(beta[3]), 
      ylab = "Profile Likelihood", 
      main = expression(paste("Profile Likelihood for ", beta[3])))

prof7= Vectorize(function(par) prof.lik(parint = par, ind = 7, init = "MLE"))
curve(prof7, 0.19,0.22 , n = 50, lwd = 1, xlab = expression(beta[4]), 
      ylab = "Profile Likelihood", 
      main = expression(paste("Profile Likelihood for ", beta[4])))

prof8 = Vectorize(function(par) prof.lik(parint = par, ind = 8, init = "MLE"))
curve(prof4, 0.5, 0.55 , n = 50, lwd = 1, xlab = expression(beta[5]), 
      ylab = "Profile Likelihood", 
      main = expression(paste("Profile Likelihood for ", beta[5])))

prof9 = Vectorize(function(par) prof.lik(parint = par, ind = 9, init = "MLE"))
curve(prof4, -0.05, 0.05 , n = 50, lwd = 1, xlab = expression(beta[6]), 
      ylab = "Profile Likelihood", 
      main = expression(paste("Profile Likelihood for ", beta[6])))
  
prof10 = Vectorize(function(par) prof.lik(parint = par, ind = 10, init = "MLE"))
curve(prof10, -0.67, -0.66 , n = 50, lwd = 1, xlab = expression(alpha[0]), 
      ylab = "Profile Likelihood", 
      main = expression(paste("Profile Likelihood for ", alpha[0])))

prof11 = Vectorize(function(par) prof.lik(parint = par, ind = 11, init = "MLE"))
curve(prof11, 0.08, 0.09 , n = 50, lwd = 1, xlab = expression(alpha[1]), 
      ylab = "Profile Likelihood", 
      main = expression(paste("Profile Likelihood for ", alpha[1])))

prof12 = Vectorize(function(par) prof.lik(parint = par, ind = 12, init = "MLE"))
curve(prof12, 0.66, 0.67 , n = 50, lwd = 1, xlab = expression(alpha[2]), 
      ylab = "Profile Likelihood", 
      main = expression(paste("Profile Likelihood for ", alpha[2])))

prof13 = Vectorize(function(par) prof.lik(parint = par, ind = 13, init = "MLE"))
curve(prof13, 0.06, 0.065, n = 50, lwd = 1, xlab = expression(alpha[3]), 
      ylab = "Profile Likelihood", 
      main = expression(paste("Profile Likelihood for ", alpha[3])))

prof14 = Vectorize(function(par) prof.lik(parint = par, ind = 14, init = "MLE"))
curve(prof14, -0.365, -0.36 , n = 50, lwd = 1, xlab = expression(alpha[4]), 
      ylab = "Profile Likelihood", 
      main = expression(paste("Profile Likelihood for ", alpha[4])))

prof15 = Vectorize(function(par) prof.lik(parint = par, ind = 15, init = "MLE"))
curve(prof15, 0.795, 0.8, n = 50, lwd = 1, xlab = expression(alpha[5]), 
      ylab = "Profile Likelihood", 
      main = expression(paste("Profile Likelihood for ", alpha[5])))

prof16 = Vectorize(function(par) prof.lik(parint = par, ind = 16, init = "MLE"))
curve(prof16, 0.17, 0.175, n = 50, lwd = 1, xlab = expression(alpha[6]), 
      ylab = "Profile Likelihood", 
      main = expression(paste("Profile Likelihood for ", alpha[6])))

prof17 = Vectorize(function(par) prof.lik(parint = par, ind = 17, init = "MLE"))
curve(prof17, 0.0025, 0.0029, n = 50, lwd = 1, xlab = expression(alpha[7]), 
      ylab = "Profile Likelihood", 
      main = expression(paste("Profile Likelihood for ", alpha[7])))

```

### Constructing a Profile Likelihood Confidence Interval
```{r}
#MODIFICATION - Changed rho to sigma
#MODIFICATION - added the variable bound to denote the 0.147 boundary point
#for our profile likelihood confidence interval. 
#Defining our significance level, SL = 0.05
SL = 0.05
#Defining bound following Lemma 4.1 in my dissertation. 
bound = exp(-0.5*qchisq(1-SL, df = 1))
#Vectorise a shifted version of the sigma parameter (parameter 2), to create a
#0.147-profile likelihood confidence interval - see Chapter 4 for more details.
intprof1 = Vectorize(function(sigma) prof1(sigma) - bound)
#Plotting this new profile log-likelihood with the curve() function.
curve(prof1, 1.20,1.35, n = 50, lwd = 1, xlab = expression(sigma), ylab = 
        "Profile Likelihood")
#... then adding the line corresponding to 0.147 to show the confidence interval
#of interest. 
abline(h = bound, lwd = 2, col = "red")

#Evaluating the lower and upper profile likelihood confidence interval, LP and
#UP respectively, for a 0.147-confidence interval for sigma, using uniroot()

LP = uniroot(f = intprof1, interval = c(1.20,1.25))$root
UP = uniroot(f = intprof1, interval = c(1.30,1.35))$root

#Getting a 0.147-profile likelihood confidence interval, which is equivalent to 
#a 0.95-confidence interval. 
c(LP, UP)
```

### Hessian Method
```{r}
#THIS IS A NEW ADDITION TO THE CODE - NOT FOUND IN RUBIO (2024). 
#Performing the procedure as described in Cole (2020), p. 
#Evaluating the Hessian at the numerically-obtained MLE. 
hess.model = -hessian(func = OPT$loglik, x = OPT$OPT$par)
#Getting the Hessian's eigenvalues.
eigen.vals = eigen(hess.model)$values
#Getting the reference values as defined in Cole (2020) and in Chapter 5 of the 
#Dissertation.
ref.vals = abs(as.vector(eigen.vals))/max(abs(as.vector(eigen.vals)))
#Getting sev as the sorted vector of reference values. 
sev = sort(ref.vals)
sev
#Checking if the larges reference value is lower than the 1E-9 threshold for 
#non-identifiability. 
sev[1] < 1E-9
```