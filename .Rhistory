confint8
MLE[8]
confint9
MLE[9]
confint10
confint11
MLE[11]
OPT0$MLE[1]
ssMLE$estimate
confint[1]
confint1
confint2
MLE.B[,2]
ssMLE.2step$rho
for (i in 1:B){
#Deciding which observation (i.e. index) from dat we will use to bootstrap,
#and using sample() to get the index to be bootstrapped over, ind.
ind = sample(1:n, replace = T)
#Using GHMLE() again to fit the model, just like for OPTPGWGH, except
#only considering the inputs corresponding to observatoin ind.
OPTB = MLETobit2(init = rep(0, npar), out = resp.obs[ind],
sel = resp.sel[ind], deso = dm.obs[ind,],
dess = dm.sel[ind,], maxit = 1E4)
MLE.B[i, ] = c(exp(OPTB$OPT$par[1]), atanh(OPTB$OPT$par[2]),
OPTB$OPT$par[-c(1:3)])
#The following code is just the implementation from above of the Hessian
#method - to avoid bloat, please refer to the comments from the preceeding
#section.,
#This was originally a bug, where it evaluated the hessian using the wrong
#optimised output - I found it, and this is mentioned in the corresponding
#RPub (https://rpubs.com/FJRubio/NRPNILung) as a disclaimer.
hessb = -hessian(OPTB$loglik, x = OPTB$OPT$par)
eigen.val.b = eigen(hessb)$values
ref.val.b = abs(as.vector(eigen.val.b))/abs(max(as.vector(eigen.val.b)))
sev.b = sort(ref.val.b)
indHess[i] = as.numeric(sev.b[1]<0.001)
}
for (i in 1:B){
#Deciding which observation (i.e. index) from dat we will use to bootstrap,
#and using sample() to get the index to be bootstrapped over, ind.
ind = sample(1:n, replace = T)
#Using GHMLE() again to fit the model, just like for OPTPGWGH, except
#only considering the inputs corresponding to observatoin ind.
OPTB = MLETobit2(init = rep(0, npar), out = resp.obs[ind],
sel = resp.sel[ind], deso = dm.obs[ind,],
dess = dm.sel[ind,], maxit = 1E4)
MLE.B[i, ] = c(exp(OPTB$OPT$par[1]), atanh(OPTB$OPT$par[2]),
OPTB$OPT$par[-c(1:2)])
#The following code is just the implementation from above of the Hessian
#method - to avoid bloat, please refer to the comments from the preceeding
#section.,
#This was originally a bug, where it evaluated the hessian using the wrong
#optimised output - I found it, and this is mentioned in the corresponding
#RPub (https://rpubs.com/FJRubio/NRPNILung) as a disclaimer.
hessb = -hessian(OPTB$loglik, x = OPTB$OPT$par)
eigen.val.b = eigen(hessb)$values
ref.val.b = abs(as.vector(eigen.val.b))/abs(max(as.vector(eigen.val.b)))
sev.b = sort(ref.val.b)
indHess[i] = as.numeric(sev.b[1]<0.001)
}
for (i in 1:B){
#Deciding which observation (i.e. index) from dat we will use to bootstrap,
#and using sample() to get the index to be bootstrapped over, ind.
ind = sample(1:n, replace = T)
#Using GHMLE() again to fit the model, just like for OPTPGWGH, except
#only considering the inputs corresponding to observatoin ind.
OPTB = MLETobit2(init = rep(0, npar), out = resp.obs[ind],
sel = resp.sel[ind], deso = dm.obs[ind,],
dess = dm.sel[ind,], maxit = 1E4)
MLE.B[i, ] = c(exp(OPTB$OPT$par[1]), tanh(OPTB$OPT$par[2]),
OPTB$OPT$par[-c(1:2)])
#The following code is just the implementation from above of the Hessian
#method - to avoid bloat, please refer to the comments from the preceeding
#section.,
#This was originally a bug, where it evaluated the hessian using the wrong
#optimised output - I found it, and this is mentioned in the corresponding
#RPub (https://rpubs.com/FJRubio/NRPNILung) as a disclaimer.
hessb = -hessian(OPTB$loglik, x = OPTB$OPT$par)
eigen.val.b = eigen(hessb)$values
ref.val.b = abs(as.vector(eigen.val.b))/abs(max(as.vector(eigen.val.b)))
sev.b = sort(ref.val.b)
indHess[i] = as.numeric(sev.b[1]<0.001)
}
for (i in 1:length(MLE)){
assign(paste("confint", i, sep = ""),
quantile(MLE.B[, i], probs = c(0.025, 0.975)))
}
confint11
confint3
confint2
MLE[2]
max(MLE.B[,2])
confint3
MLE[3]
confint4
MLE[4]
for (i in 1:B){
#Deciding which observation (i.e. index) from dat we will use to bootstrap,
#and using sample() to get the index to be bootstrapped over, ind.
ind = sample(1:n, replace = T)
#Using GHMLE() again to fit the model, just like for OPTPGWGH, except
#only considering the inputs corresponding to observatoin ind.
OPTB = MLETobit2(init = rep(1, npar), out = resp.obs[ind],
sel = resp.sel[ind], deso = dm.obs[ind,],
dess = dm.sel[ind,], maxit = 1E4)
MLE.B[i, ] = c(exp(OPTB$OPT$par[1]), tanh(OPTB$OPT$par[2]),
OPTB$OPT$par[-c(1:2)])
#The following code is just the implementation from above of the Hessian
#method - to avoid bloat, please refer to the comments from the preceeding
#section.,
#This was originally a bug, where it evaluated the hessian using the wrong
#optimised output - I found it, and this is mentioned in the corresponding
#RPub (https://rpubs.com/FJRubio/NRPNILung) as a disclaimer.
hessb = -hessian(OPTB$loglik, x = OPTB$OPT$par)
eigen.val.b = eigen(hessb)$values
ref.val.b = abs(as.vector(eigen.val.b))/abs(max(as.vector(eigen.val.b)))
sev.b = sort(ref.val.b)
indHess[i] = as.numeric(sev.b[1]<0.001)
}
mean(indHess)
for (i in 1:length(MLE)){
assign(paste("confint", i, sep = ""),
quantile(MLE.B[, i], probs = c(0.025, 0.975)))
}
confint4
MLE[4]
confint1
MLE[1]
MLE[2]
confint2
confint3
MLE[3]
confint4
MLE[4]
BC_a.ci = function(B, MLE, ind, alpha, booted.MLEs){
#Evaluate z_0.hat as in DiCiccio and Efron (1996).
z_0.hat = qnorm(1/B*sum(booted.MLEs[,ind] < MLE[ind]))
#Defining n to be the number of observations, i.e. the number of rows in our
#dataset.
n = nrow(dat)
#Evaluating the Jackknife influence functionss so we can evaluate a.hat.
jackknives = c(rep(NA, n))
#We do this iteratively.
for (i in 1:n){
#Get the MLEs for the GHPGW model, omitting the ith observation, and
#storing this in the variable OPT_i. For more details see the above section
#where we evaluated all the MLEs in the regular case.
OPT_i = MLETobit2(init = rep(1, npar), out = resp.obs[,-i],
sel = resp.sel[m-i], deso = dm.obs[,-i],
dess = dm.sel[, -i], maxit = 1E4)
#Getting the MLEs without i, as detailed above in the regular case.
MLE_i = c(exp(OPTB$OPT$par[1]), tanh(OPTB$OPT$par[2]),
OPTB$OPT$par[-c(1:2)])
jackknives[i] = (n-1)*(MLE[ind] - MLE_i[ind])
}
#Defining a.hat as in DiCaccio and Efron (1996), and evaluating the lower
#bound (LB) and upper bound (UB) of the BC_a confidence interval.
a.hat = 1/6*sum(jackknives^3)/(sum(jackknives^2)^1.5)
LB = quantile(x = booted.MLEs[,ind], probs = pnorm(z_0.hat + (z_0.hat +
qnorm(alpha/2 )/(1 - a.hat*(z_0.hat + qnorm(alpha/2) ) ) ) ) )
UB = quantile(x = booted.MLEs[,ind], probs = pnorm(z_0.hat + (z_0.hat + qnorm(1 -
alpha/2 )/(1 - a.hat*(z_0.hat + qnorm(1-alpha/2) ) ) ) ) )
#Defining the variable bootstrap.CI to be our BC_a CI.
bootstrap.CI = c(LB, UB)
#Returning bootstrap.CI.
return(bootstrap.CI)
}
BC_a.ci(B, MLE, 2, 0.05, MLE.B)
#Create the function BC_a.ci, which implements this, and takes in parameters
#B, the number of bootstrap samples, MLE, the MLE VECTOR of the data,
#ind, the index determining which MLE to create the confidence interval for,
#alpha, the significance level of the confidence interval, and booted.MLes,
#the matrix of bootstrapped MLEs.
BC_a.ci = function(B, MLE, ind, alpha, booted.MLEs){
#Evaluate z_0.hat as in DiCiccio and Efron (1996).
z_0.hat = qnorm(1/B*sum(booted.MLEs[,ind] < MLE[ind]))
#Defining n to be the number of observations, i.e. the number of rows in our
#dataset.
n = nrow(dat)
#Evaluating the Jackknife influence functionss so we can evaluate a.hat.
jackknives = c(rep(NA, n))
#We do this iteratively.
for (i in 1:n){
#Get the MLEs for the GHPGW model, omitting the ith observation, and
#storing this in the variable OPT_i. For more details see the above section
#where we evaluated all the MLEs in the regular case.
OPT_i = MLETobit2(init = rep(1, npar), out = resp.obs[-i,],
sel = resp.sel[-i,], deso = dm.obs[-i,],
dess = dm.sel[-i, ], maxit = 1E4)
#Getting the MLEs without i, as detailed above in the regular case.
MLE_i = c(exp(OPTB$OPT$par[1]), tanh(OPTB$OPT$par[2]),
OPTB$OPT$par[-c(1:2)])
jackknives[i] = (n-1)*(MLE[ind] - MLE_i[ind])
}
#Defining a.hat as in DiCaccio and Efron (1996), and evaluating the lower
#bound (LB) and upper bound (UB) of the BC_a confidence interval.
a.hat = 1/6*sum(jackknives^3)/(sum(jackknives^2)^1.5)
LB = quantile(x = booted.MLEs[,ind], probs = pnorm(z_0.hat + (z_0.hat +
qnorm(alpha/2 )/(1 - a.hat*(z_0.hat + qnorm(alpha/2) ) ) ) ) )
UB = quantile(x = booted.MLEs[,ind], probs = pnorm(z_0.hat + (z_0.hat + qnorm(1 -
alpha/2 )/(1 - a.hat*(z_0.hat + qnorm(1-alpha/2) ) ) ) ) )
#Defining the variable bootstrap.CI to be our BC_a CI.
bootstrap.CI = c(LB, UB)
#Returning bootstrap.CI.
return(bootstrap.CI)
}
BC_a.ci(B, MLE, 2, 0.05, MLE.B)
#Create the function BC_a.ci, which implements this, and takes in parameters
#B, the number of bootstrap samples, MLE, the MLE VECTOR of the data,
#ind, the index determining which MLE to create the confidence interval for,
#alpha, the significance level of the confidence interval, and booted.MLes,
#the matrix of bootstrapped MLEs.
BC_a.ci = function(B, MLE, ind, alpha, booted.MLEs){
#Evaluate z_0.hat as in DiCiccio and Efron (1996).
z_0.hat = qnorm(1/B*sum(booted.MLEs[,ind] < MLE[ind]))
#Defining n to be the number of observations, i.e. the number of rows in our
#dataset.
n = nrow(dat)
#Evaluating the Jackknife influence functionss so we can evaluate a.hat.
jackknives = c(rep(NA, n))
#We do this iteratively.
for (i in 1:n){
#Get the MLEs for the GHPGW model, omitting the ith observation, and
#storing this in the variable OPT_i. For more details see the above section
#where we evaluated all the MLEs in the regular case.
OPT_i = MLETobit2(init = rep(1, npar), out = resp.obs[-i,],
sel = resp.sel[-i, ], deso = dm.obs[-i, ],
dess = dm.sel[-i, ], maxit = 1E4)
#Getting the MLEs without i, as detailed above in the regular case.
MLE_i = c(exp(OPTB$OPT$par[1]), tanh(OPTB$OPT$par[2]),
OPTB$OPT$par[-c(1:2)])
jackknives[i] = (n-1)*(MLE[ind] - MLE_i[ind])
}
#Defining a.hat as in DiCaccio and Efron (1996), and evaluating the lower
#bound (LB) and upper bound (UB) of the BC_a confidence interval.
a.hat = 1/6*sum(jackknives^3)/(sum(jackknives^2)^1.5)
LB = quantile(x = booted.MLEs[,ind], probs = pnorm(z_0.hat + (z_0.hat +
qnorm(alpha/2 )/(1 - a.hat*(z_0.hat + qnorm(alpha/2) ) ) ) ) )
UB = quantile(x = booted.MLEs[,ind], probs = pnorm(z_0.hat + (z_0.hat + qnorm(1 -
alpha/2 )/(1 - a.hat*(z_0.hat + qnorm(1-alpha/2) ) ) ) ) )
#Defining the variable bootstrap.CI to be our BC_a CI.
bootstrap.CI = c(LB, UB)
#Returning bootstrap.CI.
return(bootstrap.CI)
}
BC_a.ci(B, MLE, 2, 0.05, MLE.B)
BC_a.ci(B, MLE, 2, 0.05, MLE.B)
OPT_i = MLETobit2(init = rep(1, npar), out = resp.obs[-i,],
sel = resp.sel[-i, ], deso = dm.obs[-i, ],
dess = dm.sel[-i, ], maxit = 1E4)
#Get the MLEs for the GHPGW model, omitting the ith observation, and
#storing this in the variable OPT_i. For more details see the above section
#where we evaluated all the MLEs in the regular case.
OPT_i = MLETobit2(init = rep(1, npar), out = resp.obs[-1,],
sel = resp.sel[-1, ], deso = dm.obs[-1, ],
dess = dm.sel[-1, ], maxit = 1E4)
resp.obs[-1, ]
#Create the function BC_a.ci, which implements this, and takes in parameters
#B, the number of bootstrap samples, MLE, the MLE VECTOR of the data,
#ind, the index determining which MLE to create the confidence interval for,
#alpha, the significance level of the confidence interval, and booted.MLes,
#the matrix of bootstrapped MLEs.
BC_a.ci = function(B, MLE, ind, alpha, booted.MLEs){
#Evaluate z_0.hat as in DiCiccio and Efron (1996).
z_0.hat = qnorm(1/B*sum(booted.MLEs[,ind] < MLE[ind]))
#Defining n to be the number of observations, i.e. the number of rows in our
#dataset.
n = nrow(dat)
#Evaluating the Jackknife influence functionss so we can evaluate a.hat.
jackknives = c(rep(NA, n))
#We do this iteratively.
for (i in 1:n){
#Get the MLEs for the GHPGW model, omitting the ith observation, and
#storing this in the variable OPT_i. For more details see the above section
#where we evaluated all the MLEs in the regular case.
OPT_i = MLETobit2(init = rep(1, npar), out = resp.obs[-i],
sel = resp.sel[-i], deso = dm.obs[-i, ],
dess = dm.sel[-i, ], maxit = 1E4)
#Getting the MLEs without i, as detailed above in the regular case.
MLE_i = c(exp(OPTB$OPT$par[1]), tanh(OPTB$OPT$par[2]),
OPTB$OPT$par[-c(1:2)])
jackknives[i] = (n-1)*(MLE[ind] - MLE_i[ind])
}
#Defining a.hat as in DiCaccio and Efron (1996), and evaluating the lower
#bound (LB) and upper bound (UB) of the BC_a confidence interval.
a.hat = 1/6*sum(jackknives^3)/(sum(jackknives^2)^1.5)
LB = quantile(x = booted.MLEs[,ind], probs = pnorm(z_0.hat + (z_0.hat +
qnorm(alpha/2 )/(1 - a.hat*(z_0.hat + qnorm(alpha/2) ) ) ) ) )
UB = quantile(x = booted.MLEs[,ind], probs = pnorm(z_0.hat + (z_0.hat + qnorm(1 -
alpha/2 )/(1 - a.hat*(z_0.hat + qnorm(1-alpha/2) ) ) ) ) )
#Defining the variable bootstrap.CI to be our BC_a CI.
bootstrap.CI = c(LB, UB)
#Returning bootstrap.CI.
return(bootstrap.CI)
}
BC_a.ci(B, MLE, 2, 0.05, MLE.B)
for (i in 1:length(MLE)){
assign(paste("confint.bc-a", i, sep = ""),
BC_a.ci(B, MLE, i, 0.05, MLE.B))
}
`confint.bc-a1`
confint.bc-a1
confint.bc-a1
'confint.bc-a1'
`confint.bc-a1`
`confint.bc-a2`
`confint.bc-a3`
`confint.bc-a4`
sys.time(BC_a.ci(B, MLE, 1, 0.05, MLE.B))
library(sys)
sys.time(BC_a.ci(B, MLE, 1, 0.05, MLE.B))
Sys.time(BC_a.ci(B, MLE, 1, 0.05, MLE.B))
BC_a.ci(B, MLE, 1, 0.05, MLE.B)
?Sys.time
Sys.time()
system.time(BC_a.ci(B, MLE, 1, 0.05, MLE.B))
#Create the function BC_a.ci, which implements this, and takes in parameters
#B, the number of bootstrap samples, MLE, the MLE VECTOR of the data,
#ind, the index determining which MLE to create the confidence interval for,
#alpha, the significance level of the confidence interval, and booted.MLes,
#the matrix of bootstrapped MLEs.
BC_a.ci = function(B, MLE, ind, alpha, booted.MLEs){
#Evaluate z_0.hat as in DiCiccio and Efron (1996).
z_0.hat = qnorm(1/B*sum(booted.MLEs[,ind] < MLE[ind]))
#Defining n to be the number of observations, i.e. the number of rows in our
#dataset.
n = nrow(dat)
#Evaluating the Jackknife influence functionss so we can evaluate a.hat.
jackknives = c(rep(NA, n))
#We do this iteratively.
for (i in 1:n){
#Get the MLEs for the GHPGW model, omitting the ith observation, and
#storing this in the variable OPT_i. For more details see the above section
#where we evaluated all the MLEs in the regular case.
OPT_i = MLETobit2(init = rep(1, npar), out = resp.obs[-i],
sel = resp.sel[-i], deso = dm.obs[-i, ],
dess = dm.sel[-i, ], maxit = 5E3)
#Getting the MLEs without i, as detailed above in the regular case.
MLE_i = c(exp(OPTB$OPT$par[1]), tanh(OPTB$OPT$par[2]),
OPTB$OPT$par[-c(1:2)])
jackknives[i] = (n-1)*(MLE[ind] - MLE_i[ind])
}
#Defining a.hat as in DiCaccio and Efron (1996), and evaluating the lower
#bound (LB) and upper bound (UB) of the BC_a confidence interval.
a.hat = 1/6*sum(jackknives^3)/(sum(jackknives^2)^1.5)
LB = quantile(x = booted.MLEs[,ind], probs = pnorm(z_0.hat + (z_0.hat +
qnorm(alpha/2 )/(1 - a.hat*(z_0.hat + qnorm(alpha/2) ) ) ) ) )
UB = quantile(x = booted.MLEs[,ind], probs = pnorm(z_0.hat + (z_0.hat + qnorm(1 -
alpha/2 )/(1 - a.hat*(z_0.hat + qnorm(1-alpha/2) ) ) ) ) )
#Defining the variable bootstrap.CI to be our BC_a CI.
bootstrap.CI = c(LB, UB)
#Returning bootstrap.CI.
return(bootstrap.CI)
}
nrow(dat)
#Create the function BC_a.ci, which implements this, and takes in parameters
#B, the number of bootstrap samples, MLE, the MLE VECTOR of the data,
#ind, the index determining which MLE to create the confidence interval for,
#alpha, the significance level of the confidence interval, and booted.MLes,
#the matrix of bootstrapped MLEs.
BC_a.ci = function(B, MLE, ind, alpha, booted.MLEs){
#Evaluate z_0.hat as in DiCiccio and Efron (1996).
z_0.hat = qnorm(1/B*sum(booted.MLEs[,ind] < MLE[ind]))
#Defining n to be the number of observations, i.e. the number of rows in our
#dataset.
n = nrow(dat)
#Evaluating the Jackknife influence functionss so we can evaluate a.hat.
jackknives = c(rep(NA, n))
#We do this iteratively.
for (i in 1:n){
#Get the MLEs for the GHPGW model, omitting the ith observation, and
#storing this in the variable OPT_i. For more details see the above section
#where we evaluated all the MLEs in the regular case.
OPT_i = MLETobit2(init = rep(1, npar), out = resp.obs[-i],
sel = resp.sel[-i], deso = dm.obs[-i, ],
dess = dm.sel[-i, ], maxit = 5E3)
#Getting the MLEs without i, as detailed above in the regular case.
MLE_i = c(exp(OPTB$OPT$par[1]), tanh(OPTB$OPT$par[2]),
OPTB$OPT$par[-c(1:2)])
jackknives[i] = (n-1)*(MLE[ind] - MLE_i[ind])
}
#Defining a.hat as in DiCaccio and Efron (1996), and evaluating the lower
#bound (LB) and upper bound (UB) of the BC_a confidence interval.
a.hat = 1/6*sum(jackknives^3)/(sum(jackknives^2)^1.5)
LB = quantile(x = booted.MLEs[,ind], probs = pnorm(z_0.hat + (z_0.hat +
qnorm(alpha/2 )/(1 - a.hat*(z_0.hat + qnorm(alpha/2) ) ) ) ) )
UB = quantile(x = booted.MLEs[,ind], probs = pnorm(z_0.hat + (z_0.hat + qnorm(1 -
alpha/2 )/(1 - a.hat*(z_0.hat + qnorm(1-alpha/2) ) ) ) ) )
#Defining the variable bootstrap.CI to be our BC_a CI.
bootstrap.CI = c(LB, UB)
#Returning bootstrap.CI.
return(bootstrap.CI)
}
#WARNING: This loop takes a VERY long time to run.
#If you are viewing this, I would HEAVILY recommend AGAINST running this for
#loop. This is due to how the Jackknife is calculated.
for (i in 1:length(MLE)){
assign(paste("confint.bc_a", i, sep = ""),
BC_a.ci(B, MLE, i, 0.05, MLE.B))
}
#Create the function BC_a.ci, which implements this, and takes in parameters
#B, the number of bootstrap samples, MLE, the MLE VECTOR of the data,
#ind, the index determining which MLE to create the confidence interval for,
#alpha, the significance level of the confidence interval, and booted.MLes,
#the matrix of bootstrapped MLEs.
BC_a.ci = function(B, MLE, ind, alpha, booted.MLEs){
#Evaluate z_0.hat as in DiCiccio and Efron (1996).
z_0.hat = qnorm(1/B*sum(booted.MLEs[,ind] < MLE[ind]))
#Defining n to be the number of observations, i.e. the number of rows in our
#dataset.
n = nrow(dat)
#Evaluating the Jackknife influence functionss so we can evaluate a.hat.
jackknives = c(rep(NA, n))
#We do this iteratively.
for (i in 1:n){
#Get the MLEs for the GHPGW model, omitting the ith observation, and
#storing this in the variable OPT_i. For more details see the above section
#where we evaluated all the MLEs in the regular case.
OPT_i = MLETobit2(init = rep(1, npar), out = resp.obs[-i],
sel = resp.sel[-i], deso = dm.obs[-i, ],
dess = dm.sel[-i, ], maxit = 1E4)
#Getting the MLEs without i, as detailed above in the regular case.
MLE_i = c(exp(OPTB$OPT$par[1]), tanh(OPTB$OPT$par[2]),
OPTB$OPT$par[-c(1:2)])
jackknives[i] = (n-1)*(MLE[ind] - MLE_i[ind])
}
#Defining a.hat as in DiCaccio and Efron (1996), and evaluating the lower
#bound (LB) and upper bound (UB) of the BC_a confidence interval.
a.hat = 1/6*sum(jackknives^3)/(sum(jackknives^2)^1.5)
LB = quantile(x = booted.MLEs[,ind], probs = pnorm(z_0.hat + (z_0.hat +
qnorm(alpha/2 )/(1 - a.hat*(z_0.hat + qnorm(alpha/2) ) ) ) ) )
UB = quantile(x = booted.MLEs[,ind], probs = pnorm(z_0.hat + (z_0.hat + qnorm(1 -
alpha/2 )/(1 - a.hat*(z_0.hat + qnorm(1-alpha/2) ) ) ) ) )
#Defining the variable bootstrap.CI to be our BC_a CI.
bootstrap.CI = c(LB, UB)
#Returning bootstrap.CI.
return(bootstrap.CI)
}
#WARNING: This loop takes a VERY long time to run.
#If you are viewing this, I would HEAVILY recommend AGAINST running this for
#loop. This is due to how the Jackknife is calculated.
for (i in 1:length(MLE)){
assign(paste("confint.bc_a", i, sep = ""),
BC_a.ci(B, MLE, i, 0.05, MLE.B))
}
for (i in 10:length(MLE)){
assign(paste("confint.bc_a", i, sep = ""),
BC_a.ci(B, MLE, i, 0.05, MLE.B))
}
confint.bc_a10
MLE[10]
OPT$MLE[9]
prof9 = Vectorize(function(par) prof.lik(parint = par, ind = 9, init = "MLE",
method = "nlminb"))
curve(prof9, n = 100, from = -2, to = -1, ylab = "Profile Likelihood",
xlab = expression(beta[5]))
abline(v = OPT$MLE[9], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")
MLE[9]
prof9 = Vectorize(function(par) prof.lik(parint = par, ind = 10, init = "MLE",
method = "nlminb"))
curve(prof9, n = 100, from = -2, to = -1, ylab = "Profile Likelihood",
xlab = expression(alpha[1]))
prof9 = Vectorize(function(par) prof.lik(parint = par, ind = 9, init = "MLE",
method = "nlminb"))
prof10 = Vectorize(function(par) prof.lik(parint = par, ind = 10, init = "MLE",
method = "nlminb"))
MLE[10]
prof10 = Vectorize(function(par) prof.lik(parint = par, ind = 10, init = "MLE",
method = "nlminb"))
curve(prof10, n = 100, from = 0, to = 0.2, ylab = "Profile Likelihood",
xlab = expression(alpha[1]))
abline(v = OPT$MLE[10], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")
OPT$MLE[11]
prof11 = Vectorize(function(par) prof.lik(parint = par, ind = 11, init = "MLE",
method = "nlminb"))
curve(prof11, n = 100, from = -0.001, to = 0, ylab = "Profile Likelihood",
xlab = expression(alpha[2]))
abline(v = OPT$MLE[11], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")
OPT$MLE[12]
prof12 = Vectorize(function(par) prof.lik(parint = par, ind = 12, init = "MLE",
method = "nlminb"))
curve(prof10, n = 100, from = 0, to = 0.2, ylab = "Profile Likelihood",
xlab = expression(alpha[3]))
curve(prof12, n = 100, from = 0, to = 0.2, ylab = "Profile Likelihood",
xlab = expression(alpha[3]))
abline(v = OPT$MLE[12], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")
curve(prof13, n = 100, from = 0, to = 1, ylab = "Profile Likelihood",
xlab = expression(alpha[3]))
curve(prof12, n = 100, from = 0, to = 1, ylab = "Profile Likelihood",
xlab = expression(alpha[3]))
abline(v = OPT$MLE[12], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")
OPT$MLE[12]
curve(prof12, n = 100, from = 0.14, to = 0.15, ylab = "Profile Likelihood",
xlab = expression(alpha[3]))
abline(v = OPT$MLE[12], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")
OPT$MLE[13]
prof13 = Vectorize(function(par) prof.lik(parint = par, ind = 13, init = "MLE",
method = "nlminb"))
curve(prof13, n = 100, from = 0.01, to = 0.03, ylab = "Profile Likelihood",
xlab = expression(alpha[3]))
abline(v = OPT$MLE[13], col = "red", lwd = 2)
legend(x = "topleft", legend = c("MLE"), pch = 16, col = "red")
confint4
MLE[4]
confint13
MLE[13]
