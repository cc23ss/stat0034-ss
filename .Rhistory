knitr::opts_chunk$set(echo = TRUE)
#The following implementation was taken from Rubio (2024)
#ADDITION: setting the seed for replicability.
set.seed(1111)
#Importing the sampleSelection package - This is the focal package of my
#dissertation.
library(sampleSelection)
#Importing the libraries ssmrob and ssmodels, which are for robust inference and
#modelling for sample selection models.
library(ssmrob)
library(ssmodels)
#Defining the MLE for Tobit-2 model as the function MLETobit2, as discussed in
#Chapter 2 of my dissertation.
#This takes inputs init, the inital value from where the parameter estimate is
#evaluated from, out, the responses of the observed model, sel, the responses
#of the selection model, deso, the design matrix for the observed model, less the
#first column, dess, the design matrix for the selection process less the first
#column, method, the optimisation method, and maxit, the maximum number of
#iterations, with default number 100.
MLETobit2 = function(init, out, sel, deso, dess, beta_par = 0,
method = "nlminb", maxit = 100){
#Converting the inputs out and sel into vectors.
out = as.vector(out); sel = as.vector(sel)
#Defining x1 and w1 as the design matrices of the observed and selection
#processes respectively by adding back the missing column of 1's.
x1 = as.matrix(cbind(1, deso)); w1 = as.matrix(cbind(1, dess))
#Defining p and q as the number of columns of x1 and w1 respectively.
p = ncol(x1); q = ncol(w1)
#Defining a function which evaluates the log-likelihood of the Tobit-2 model,
#loglik. This takes in as inputs a parameter vector, par.
loglik = function(par){
#Reparametrising sigma and rho in our model, using an exponential and a
#tanh link.
sigma = exp(par[1]); rho = tanh(par[2])
#Defining our linear regression coefficients for the observed model, beta,
#and that for the selection model, alpha.
#TODO: Reparametrise beta = beta.tilde
beta = par[c(3:(p+2))]; alpha = par[(p+3):(p+q+2)]
#Reparametrisation if beta_par = 1
if(beta_par){
beta = beta/sigma
}
x_beta = x1%*%beta; w_alpha = w1%*%alpha
#Defining inds0 and inda1 as indicators corresponding to the selection
#criterion, which can be seen in chapter 2.
inds0 = (sel == 0); inds1 = (sel == 1)
n1 = sum(inds1)
#Evaluating the log-likelihood of the Tobit-2 model as given in Chapter 2.
ll = sum(pnorm(-w_alpha[inds0], log = T)) +
sum(pnorm( (w_alpha[inds1] + rho*(out[inds1] - x_beta[inds1])/sigma)/
sqrt(1 - rho^2), log = T)) +
#MODIFICATION: compressed the explicit evaluation of the normal component of
#the observed processes into the dnorm function, as the distribution is a
#N(x_beta, sigma^2) distributed random variable from proof (INSERT).
sum(dnorm(out[inds1], mean = x_beta[inds1], sd = sigma, log = T))
#Returning the log-loss, which is the objective we seek to minimise.
return(-ll)
}
#Conditional to evaluate the optimal value using nlminb if specified
#(by default).
if(method == "nlminb"){
OPT = nlminb(init, loglik, control = list(iter.max = maxit))
}
#Conditional to evaluate the optimal value using the optim() function
else{
#Using optim to
OPT = optim(init, loglik, control = list(maxit = maxit), method = method)
}
#Storing the MLES in a vector, remembering to undo the links defined above.
MLE = c(exp(OPT$par[1]), tanh(OPT$par[2]), OPT$par[-c(1,2)])
#Creating a vector of estimate names...
names(MLE) = c("sigma", "rho", "intercept", paste(paste("beta_hat[,", 1:(p-1),
sep = "" ),"]", sep = ""), "intercept", paste(paste("alpha_hat[,", 1:(q-1),
sep = "" ),"]", sep = ""))
#Storing all our outputs in the list outf.
outf = list(loglik = loglik, OPT = OPT, MLE = MLE)
#Returning outf as the function output.
return(outf)
}
#Attach the MEPS2001 dataset from the ssmrob package.
attach(MEPS2001)
#Beginning of exploratory data analysis.
#?MEPS2001 - Read documentation for this dataset.
#Look at the top of
head(MEPS2001)
#TODO: Exploratory data analysis.
#Defining design matrices (less the intercept column).
#TODO: Change influential variables?
#... for the observed model.
dm.obs = cbind(age, female, educ, blhisp, totchr, ins)
boxplot(dm.obs[, c(1, 3, 5)])
pairs(dm.obs[, c(1, 3, 5)])
summary(dm.obs)
#... for the selection model.
#TODO: Change the issue of double-counting covariates here.
dm.sel1 = cbind(dm.obs, income)
boxplot(dm.sel1[, c(1, 3, 5, 7)])
pairs(dm.sel1[, c(1, 3, 5, 7)])
summary(dm.sel1)
#TODO: EDA
#... for the selection model, with no overlapping covariates
dm.sel2 = MEPS2001[, 1]
#Selection response, a binary (0/1) variable which acts as the indicator
#function corresponding to the latent variable for positive ambulatory
#expenditures.
sel = as.logical(dambexp)
#Observed responsses, corresponding to log ambulatory expenditures, lnambx.
y = lnambx
OPT = MLETobit2(init = rep(0, 17), out = y, sel = sel, deso = dm.obs,
dess = dm.sel1, method = "nlminb", maxit = 1E4, beta_par = 0)
MLE = OPT$MLE
MLE
#Comparing this to other optimisation algorithms.
selectEq = dambexp ~ age + female + educ + blhisp + totchr + ins + income
obsEq1 = update(selectEq, . - dambexp + y ~ . - income)
HMLE = HeckmanCL(selectEq, obsEq1, data = MEPS2001)
summary(HMLE)
#TODO: Second optimiser.
ssMLE = selection(selectEq, obsEq1)
summary(ssMLE)
po = ncol(dm.obs); ps = ncol(dm.sel2)
detach(MEPS2001)
#Attach the MEPS2001 dataset from the ssmrob package.
attach(MEPS2001)
?replace(tempv)
20*(1 == 0)
prof1 = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, init = "zero"))
prof.lik = function(parint, ind, init = "zero"){
#TODO: I don't like this, and I want this out.
#MODIFICATION: Streamlined the tempf function.
tempf = function(par){
tempv = rep(par, p); tempv[ind] = parint
return(OPT$loglik(tempv))
}
#MODIFICATION - improved conditional here.
#TODO: improve this implementation.
out = -nlminb(OPT$OPT$par[-ind]*(init = "MLE"), tempf,
control = list((iter.max = 1E4))$objective + ML)
return(out)
}
prof1 = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, init = "zero"))
curve(prof1, 1.2, 1.35, n = 50, lwd = 2)
prof1 = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, init = "zero"))
curve(prof1, 1.2, 1.35, n = 50, lwd = 2)
OPT$OPT
OPT$OPT$par
OPT$OPT$par[-1]
prof.lik = function(parint, ind, init = "zero"){
#TODO: I don't like this, and I want this out.
#MODIFICATION: Streamlined the tempf function.
tempf = function(par){
tempv = rep(par, p); tempv[ind] = parint
return(OPT$loglik(tempv))
}
#MODIFICATION - improved conditional here.
#TODO: improve this implementation.
out = -nlminb(OPT$OPT$par[-ind]*rep(init = "MLE", length(OPT$OPT$par[-ind])),
tempf, control = list((iter.max = 1E4))$objective + ML)
return(out)
}
prof1 = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, init = "zero"))
curve(prof1, 1.2, 1.35, n = 50, lwd = 2)
ML = OPT$OPT$objective
prof.lik = function(parint, ind, init = "zero"){
#TODO: I don't like this, and I want this out.
#MODIFICATION: Streamlined the tempf function.
tempf = function(par){
tempv = rep(par, p); tempv[ind] = parint
return(OPT$loglik(tempv))
}
#MODIFICATION - improved conditional here.
#TODO: improve this implementation.
out = -nlminb(OPT$OPT$par[-ind]*rep(init = "MLE", length(OPT$OPT$par[-ind])),
tempf, control = list((iter.max = 1E4))$objective + ML)
return(out)
}
prof1 = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, init = "zero"))
curve(prof1, 1.2, 1.35, n = 50, lwd = 2)
p = length(MLE)
ML = OPT$OPT$objective
prof.lik = function(parint, ind, init = "zero"){
#TODO: I don't like this, and I want this out.
#MODIFICATION: Streamlined the tempf function.
tempf = function(par){
tempv = rep(par, p); tempv[ind] = parint
return(OPT$loglik(tempv))
}
#MODIFICATION - improved conditional here.
#TODO: improve this implementation.
out = -nlminb(OPT$OPT$par[-ind]*rep(init = "MLE", length(OPT$OPT$par[-ind])),
tempf, control = list((iter.max = 1E4))$objective + ML)
return(out)
}
prof1 = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, init = "zero"))
curve(prof1, 1.2, 1.35, n = 50, lwd = 2)
curve(prof1, 1.2, 1.35, n = 50, lwd = 2)
prof.lik = function(parint, ind, init = "zero"){
#TODO: I don't like this, and I want this out.
#MODIFICATION: Streamlined the tempf function.
tempf = function(par){
tempv = rep(par, p); tempv[ind] = parint
return(OPT$loglik(tempv))
}
#MODIFICATION - improved conditional here.
#TODO: improve this implementation.
out = -nlminb(OPT$OPT$par[-ind]*rep(init == "MLE", length(OPT$OPT$par[-ind])),
tempf, control = list((iter.max = 1E4))$objective + ML)
return(out)
}
prof1 = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, init = "zero"))
curve(prof1, 1.2, 1.35, n = 50, lwd = 2)
prof.lik = function(parint, ind, init = "zero"){
#TODO: I don't like this, and I want this out.
#MODIFICATION: Streamlined the tempf function.
tempf = function(par){
tempv = rep(par, p); tempv[ind] = parint
return(OPT$loglik(tempv))
}
#MODIFICATION - improved conditional here.
#TODO: improve this implementation.
print(OPT$OPT$par[-ind]*rep(init == "MLE", length(OPT$OPT$par[-ind])))
out = -nlminb(OPT$OPT$par[-ind]*rep(init == "MLE", length(OPT$OPT$par[-ind])),
tempf, control = list((iter.max = 1E4))$objective + ML)
return(out)
}
prof1 = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, init = "zero"))
curve(prof1, 1.2, 1.35, n = 50, lwd = 2)
prof.lik = function(parint, ind, init = "zero"){
#TODO: I don't like this, and I want this out.
#MODIFICATION: Streamlined the tempf function.
tempf = function(par){
tempv = rep(par, p); tempv[ind] = parint
return(OPT$loglik(tempv))
}
#MODIFICATION - improved conditional here.
#TODO: improve this implementation.
out = -nlminb(OPT$OPT$par[-ind]*rep(init == "MLE", length(OPT$OPT$par[-ind])),
tempf, control = list(iter.max = 1E4))$objective + ML
return(out)
}
prof1 = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, init = "zero"))
curve(prof1, 1.2, 1.35, n = 50, lwd = 2)
print(prof1)
print(prof1())
curve(prof1, 1.2, 1.35, n = 50, lwd = 2,xlab = expression(sigma), ylab = "Profile Likelihood")
tempf = function(par){
tempv = rep(par, p); tempv[ind] = parint
return(OPT$loglik(tempv))
}
#MODIFICATION - improved conditional here.
#TODO: improve this implementation.
#TODO: What even is this code trying to do???
out = -nlminb(OPT$OPT$par[-ind]*rep(init == "MLE", length(OPT$OPT$par[-ind])),
tempf, control = list(iter.max = 1E4))$objective + ML
par = 1
parint = 2
ind = 3
tempv <- rep(0,p)
tempv <- replace(x = tempv, c(1:p)[-ind] , par)
tempv <- replace(x = tempv, ind , parint)
tempv
tempv = rep(par, p); tempv[ind] = parint
tempv
prof.lik = function(parint, ind, init = "zero"){
#TODO: I don't like this, and I want this out.
#MODIFICATION: Streamlined the tempf function.
tempf = function(par){
tempv = rep(par, p); tempv[ind] = parint
return(OPT$loglik(tempv))
}
#MODIFICATION - improved conditional here.
#TODO: improve this implementation.
#TODO: What even is this code trying to do???
out = -nlminb(OPT$OPT$par[-ind]*rep(init == "MLE", length(OPT$OPT$par[-ind])),
tempf, control = list(iter.max = 1E4))$objective + ML
return(exp(out))
}
curve(prof1, 1.2, 1.35, n = 50, lwd = 2,xlab = expression(sigma), ylab = "Profile Likelihood")
prof1 = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, init = "zero"))
curve(prof1, 1.2, 1.35, n = 50, lwd = 2,xlab = expression(sigma), ylab = "Profile Likelihood")
#AYYYYYYY LET'S GO
curve(prof1, 1.2, 1.35, n = 50, lwd = 2,xlab = expression(sigma), ylab = "Profile Likelihood")
prof1 = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, init = "MLE"))
#AYYYYYYY LET'S GO
#Plots the profile likelihood. NOTE: This takes several minutes to run.
curve(prof1, 1.2, 1.35, n = 50, lwd = 2,xlab = expression(sigma), ylab = "Profile Likelihood")
#AYYYYYYY LET'S GO
#Plots the profile likelihood. NOTE: This takes several minutes to run.
curve(prof1, 1.2, 1.35, n = 50, lwd = 2,xlab = expression(sigma), ylab = "Profile Likelihood")
prof1 = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, init = "MLE"))
AYYYYYYY LET'S GO
prof1a = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, init = "MLE"))
curve(prof1a, 1.2, 1.35, n = 50, lwd = 2,xlab = expression(sigma), ylab = "Profile Likelihood")
prof2 <- Vectorize(function(par) prof.lik(parint = atanh(par), ind = 2, init = "MLE"))
curve(prof2,-0.6,0.25 , n = 50, lwd = 2, xlab = expression(rho), ylab = "Profile Likelihood")
prof.lik = function(parint, ind, init = "zero", max.iter = 1E4){
#MODIFICATION: Streamlined the tempf function from 4 lines to 3 lines by
#directly manipulating the tempv dummy variable.
tempf = function(par){
tempv = rep(par, p); tempv[ind] = parint
return(OPT$loglik(tempv))
}
#MODIFICATION - condensed the two conditional statements into a single line
#using truthy values & their correspondence to 0/1.
out = -nlminb(OPT$OPT$par[-ind]*rep(init == "MLE", length(OPT$OPT$par[-ind])),
tempf, control = list(iter.max = max.iter))$objective + ML
#We return the exponential of the output since we used the log-likelihood.
#This returns the profile likelihood.
return(exp(out))
}
#Create the profile likelihood for
prof1 = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, init = "zero"))
prof1b = Vectorize(function(par) prof.lik(parint = log(par), ind = 1, init = "MLE",
max.iter = 1E5))
curve(prof1b, 1.2, 1.35, n = 50, lwd = 2, xlab = expression(sigma), ylab = "Profile Likelihood")
?curve
?nlminb
?Vectorize
rho
tanh(c(LP, UP))
LP <- uniroot(f = intprof2, interval = c(-0.5,-0.4))$root
#Vectorise
intprof2 = Vectorize(function(rho) prof2(rho) - 0.147)
curve(prof2,-0.6,0.25 , n = 50, lwd = 2, xlab = expression(rho), ylab = "Profile Likelihood")
curve(prof2,-0.6,0.25 , n = 50, lwd = 2, xlab = expression(rho), ylab = "Profile Likelihood")
LP <- uniroot(f = intprof2, interval = c(-0.5,-0.4))$root
LP <- uniroot(f = intprof2, interval = c(-0.5,-0.4))$root
#Vectorise
intprof2 = Vectorize(function(rho) prof2(rho) - 0.147)
curve(intprof2,-0.6,0.25 , n = 50, lwd = 2, xlab = expression(rho), ylab = "Profile Likelihood")
curve(intprof2,-0.6,0.25 , n = 50, lwd = 2, xlab = expression(rho), ylab = "Profile Likelihood")
LP <- uniroot(f = intprof2, interval = c(-0.5,-0.4))$root
#Vectorise a shifted version of the
intprof2 = Vectorize(function(rho) prof.lik(rho) - 0.147)
LP = uniroot(f = intprof2, interval = c(-0.5,-0.4))$root
UP = uniroot(f = intprof2, interval = c(0.1,0.2))$root
